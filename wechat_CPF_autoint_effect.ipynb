{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:05.059077: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-29 13:05:05.090907: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 13:05:05.090924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 13:05:05.091901: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-29 13:05:05.097849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 13:05:05.821378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_223052/2475276552.py:15: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:06.521851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n",
      "2024-07-29 13:05:06.528272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "def seed_tensorflow(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_train_label\n",
    "# short_play\n",
    "# effective_play\n",
    "# long_play\n",
    "# complete_play\n",
    "select_label = 'short_play'\n",
    "\n",
    "# base_model = \"./model/kuairand_backbone/mlp_{}.h5\".format(select_label)\n",
    "base_model = \"./model/wechat_base/autoint_{}.h5\".format(select_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = {'data_path':'./wechat_input/',\n",
    "        'embedding_dim':64,\n",
    "        'seed':0,\n",
    "        'lr':1e-5,\n",
    "        'batch_size':512,\n",
    "        'epochs':64,\n",
    "        'verbose':1,\n",
    "        'callback':{\n",
    "                     'monitor':'val_auc',\n",
    "                     'patience':10,\n",
    "                     'base_model':base_model\n",
    "                     },\n",
    "        'mlp_dims':[256,128,64],\n",
    "        'mlp_act':'relu',\n",
    "        'mlp_dps':[.5,.5,.5],\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = pd.read_csv(para['data_path']+\"action.csv\")\n",
    "feed_emb1 = np.load(para['data_path']+\"feedid_emb_64.npy\")\n",
    "feed_emb2 = np.load(para['data_path']+\"embeddings_array_wechat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>date_</th>\n",
       "      <th>device</th>\n",
       "      <th>read_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>play</th>\n",
       "      <th>stay</th>\n",
       "      <th>click_avatar</th>\n",
       "      <th>...</th>\n",
       "      <th>videoplayseconds</th>\n",
       "      <th>pcr</th>\n",
       "      <th>duration_level</th>\n",
       "      <th>threshold</th>\n",
       "      <th>binary_train_label</th>\n",
       "      <th>short_play</th>\n",
       "      <th>effective_play</th>\n",
       "      <th>long_play</th>\n",
       "      <th>complete_play</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>63138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>1533</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>43011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>1302</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>23367</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>25886</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210182</th>\n",
       "      <td>19999</td>\n",
       "      <td>62197</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6369</td>\n",
       "      <td>6566</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.219621</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210183</th>\n",
       "      <td>19999</td>\n",
       "      <td>44479</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13212</td>\n",
       "      <td>13708</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210184</th>\n",
       "      <td>19999</td>\n",
       "      <td>43644</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32215</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210185</th>\n",
       "      <td>19999</td>\n",
       "      <td>40090</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5618</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210186</th>\n",
       "      <td>19999</td>\n",
       "      <td>45995</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160560</td>\n",
       "      <td>161024</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7210187 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  feedid  date_  device  read_comment  comment  like    play  \\\n",
       "0             0   63138      1       1             0        0     0     250   \n",
       "1             0   43011      1       1             0        0     0     750   \n",
       "2             0   23367      1       1             0        0     0     250   \n",
       "3             0   25886      1       1             0        0     0       0   \n",
       "4             0     983      1       1             0        0     0     250   \n",
       "...         ...     ...    ...     ...           ...      ...   ...     ...   \n",
       "7210182   19999   62197     12       2             0        0     0    6369   \n",
       "7210183   19999   44479     12       2             0        0     0   13212   \n",
       "7210184   19999   43644     12       2             0        0     0       0   \n",
       "7210185   19999   40090     12       2             0        0     0       0   \n",
       "7210186   19999   45995     12       2             1        0     0  160560   \n",
       "\n",
       "           stay  click_avatar  ...  videoplayseconds       pcr  \\\n",
       "0          1533             0  ...                16  0.015625   \n",
       "1          1302             0  ...                31  0.024194   \n",
       "2           800             0  ...                12  0.020833   \n",
       "3          1496             0  ...                16  0.000000   \n",
       "4           976             0  ...                19  0.013158   \n",
       "...         ...           ...  ...               ...       ...   \n",
       "7210182    6566             0  ...                29  0.219621   \n",
       "7210183   13708             0  ...                11  1.000000   \n",
       "7210184   32215             0  ...                29  0.000000   \n",
       "7210185    5618             0  ...                12  0.000000   \n",
       "7210186  161024             0  ...                60  1.000000   \n",
       "\n",
       "         duration_level  threshold  binary_train_label  short_play  \\\n",
       "0                     1   0.648438                   0           1   \n",
       "1                     2   0.625000                   0           1   \n",
       "2                     0   0.656250                   0           1   \n",
       "3                     1   0.648438                   0           1   \n",
       "4                     1   0.648438                   0           1   \n",
       "...                 ...        ...                 ...         ...   \n",
       "7210182               2   0.625000                   0           1   \n",
       "7210183               0   0.656250                   1           0   \n",
       "7210184               2   0.625000                   0           1   \n",
       "7210185               0   0.656250                   0           1   \n",
       "7210186               4   0.562500                   1           0   \n",
       "\n",
       "         effective_play  long_play  complete_play  test_label  \n",
       "0                     0          0              0           0  \n",
       "1                     0          0              0           0  \n",
       "2                     0          0              0           0  \n",
       "3                     0          0              0           0  \n",
       "4                     0          0              0           0  \n",
       "...                 ...        ...            ...         ...  \n",
       "7210182               0          0              0           0  \n",
       "7210183               1          1              1           0  \n",
       "7210184               0          0              0           0  \n",
       "7210185               0          0              0           0  \n",
       "7210186               1          1              1           1  \n",
       "\n",
       "[7210187 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = action.sort_values(by=['date_'])\n",
    "\n",
    "train= action.groupby('userid').apply(lambda x: x[:int(len(x)*0.6)]).reset_index(drop=True)\n",
    "valid= action.groupby('userid').apply(lambda x: x[int(len(x)*0.6):int(len(x)*0.8)]).reset_index(drop=True)\n",
    "test= action.groupby('userid').apply(lambda x: x[int(len(x)*0.8):]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feats = ['userid','device','feedid','authorid','duration_level','feedid','feedid',select_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_x = [train[f].values for f in sparse_feats]\n",
    "val_sparse_x = [valid[f].values for f in sparse_feats]\n",
    "test_sparse_x = [test[f].values for f in sparse_feats]\n",
    "\n",
    "train_label = [train[select_label].values]\n",
    "val_label = [valid[select_label].values]\n",
    "test_label = [test[select_label].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0,     0,     0, ..., 19999, 19999, 19999]),\n",
       " array([1, 1, 1, ..., 2, 2, 2]),\n",
       " array([63138, 60079, 72045, ..., 76447, 73900,  3543]),\n",
       " array([ 1412,  8137, 11063, ..., 17608,  4653, 14210]),\n",
       " array([1, 0, 1, ..., 3, 4, 3]),\n",
       " array([63138, 60079, 72045, ..., 76447, 73900,  3543]),\n",
       " array([63138, 60079, 72045, ..., 76447, 73900,  3543]),\n",
       " array([1, 1, 1, ..., 1, 0, 1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:32.805547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "features = ['userid','device','feedid','authorid','feedid','duration_level','feedid',select_label]\n",
    "\n",
    "# # short_play binary_train_label effective_play long_play complete_play\n",
    "def get_input(df,is_test=False):\n",
    "    X = []\n",
    "    for f in features:\n",
    "        X.append(df[f].values.reshape(-1,1))\n",
    "    y = [df[select_label].values.reshape(-1,1)]\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = get_input(train,is_test=False)\n",
    "X_valid,y_valid = get_input(valid,is_test=False)\n",
    "X_test,y_test = get_input(test,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_inputs = []\n",
    "'userid','device','feedid','authorid',\n",
    "'feedid','duration_level','',select_label\n",
    "input0 = tf.keras.layers.Input([1], name='userid',dtype=tf.int32)\n",
    "input1 = tf.keras.layers.Input([1], name='device',dtype=tf.int32)\n",
    "input2 = tf.keras.layers.Input([1], name='feedid',dtype=tf.int32)\n",
    "input3 = tf.keras.layers.Input([1], name='authorid',dtype=tf.int32)\n",
    "input4 = tf.keras.layers.Input([1], name='duration_level',dtype=tf.int32)\n",
    "input5 = tf.keras.layers.Input([1], name='feedid1',dtype=tf.int32)\n",
    "input6 = tf.keras.layers.Input([1], name='feedid2',dtype=tf.int32)\n",
    "input7 = tf.keras.layers.Input([1], name=select_label,dtype=tf.int32)\n",
    "\n",
    "sparse_inputs.append(input0)\n",
    "sparse_inputs.append(input1)\n",
    "sparse_inputs.append(input2)\n",
    "\n",
    "sparse_inputs.append(input3)\n",
    "sparse_inputs.append(input4)\n",
    "sparse_inputs.append(input5)\n",
    "\n",
    "sparse_inputs.append(input6)\n",
    "sparse_inputs.append(input7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:32.868006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "sparse_kd_embed = []\n",
    "for i, _input in enumerate(sparse_inputs):\n",
    "    if i == 5:\n",
    "        tf.keras.layers.Embedding(input_dim=int(feed_emb1.shape[0]),\n",
    "                       output_dim=int(feed_emb1.shape[1]),\n",
    "                       weights=[feed_emb1],\n",
    "                       trainable=False,\n",
    "                       name='pre_embedding1')\n",
    "        sparse_kd_embed.append(_embed)\n",
    "    elif i == 6:\n",
    "        tf.keras.layers.Embedding(input_dim=int(feed_emb2.shape[0]),\n",
    "                       output_dim=int(feed_emb2.shape[1]),\n",
    "                       weights=[feed_emb2],\n",
    "                       trainable=False,\n",
    "                       name='pre_embedding2')\n",
    "        sparse_kd_embed.append(_embed)\n",
    "    elif i==7:\n",
    "        continue\n",
    "    else:\n",
    "        f = sparse_feats[i]\n",
    "        voc_size = action[f].nunique()\n",
    "        _embed = tf.keras.layers.Embedding(voc_size+1, 64, embeddings_regularizer=tf.keras.regularizers.l2(0.5))(_input)\n",
    "        sparse_kd_embed.append(_embed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_1')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_2')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_3')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_4')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_4')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_4')>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_kd_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = sparse_kd_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_map = tf.keras.layers.Concatenate(axis=1)(input_embeds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 7, 64) dtype=float32 (created by layer 'concatenate')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_interacting(embed_map, d=6, n_attention_head=2):\n",
    "    \"\"\"\n",
    "    实现单层 AutoInt Interacting Layer\n",
    "    @param embed_map: 输入的embedding feature map, (?, n_feats, n_dim)\n",
    "    @param d: Q,K,V映射后的维度\n",
    "    @param n_attention_head: multi-head attention的个数\n",
    "    \"\"\"\n",
    "    assert len(embed_map.shape) == 3, \"Input embedding feature map must be 3-D tensor.\"\n",
    "    \n",
    "    k = embed_map.shape[-1]\n",
    "    \n",
    "    # 存储多个self-attention的结果\n",
    "    attention_heads = []\n",
    "    W_Q = []\n",
    "    W_K = []\n",
    "    W_V = []\n",
    "    \n",
    "    # 1.构建多个attention\n",
    "    for i in range(n_attention_head):\n",
    "        # 初始化W_Q, W_K, W_V\n",
    "        W_Q.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"query_\"+str(i)))  # k, d\n",
    "        W_K.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"key_\"+str(i)))  # k, d\n",
    "        W_V.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"value_\"+str(i)))  # k, d\n",
    "     \n",
    "    for i in range(n_attention_head):\n",
    "        # 映射到d维空间\n",
    "        embed_q = tf.matmul(embed_map, W_Q[i])  # ?, 39, d\n",
    "        embed_k = tf.matmul(embed_map, W_K[i])  # ?, 39, d\n",
    "        embed_v = tf.matmul(embed_map, W_V[i])  # ?, 39, d\n",
    "    \n",
    "        # 计算attention\n",
    "        energy = tf.matmul(embed_q, tf.transpose(embed_k, [0, 2, 1]))  # ?, 39, 39\n",
    "        attention = tf.nn.softmax(energy)  # ?, 39, 39\n",
    "    \n",
    "        attention_output = tf.matmul(attention, embed_v)  # ?, 39, d\n",
    "        attention_heads.append(attention_output)\n",
    "    \n",
    "    # 2.concat multi head\n",
    "    multi_attention_output = tf.keras.layers.Concatenate(axis=-1)(attention_heads)  # ?, 39, n_attention_head*d\n",
    "    \n",
    "    # 3.ResNet\n",
    "    w_res = tf.Variable(tf.random.truncated_normal(shape=(k, d*n_attention_head)), name=\"w_res_\"+str(i))  # k, d*n_attention_head\n",
    "    output = tf.keras.layers.Activation(\"relu\")(multi_attention_output + tf.matmul(embed_map, w_res))  # ?, 39, d*n_attention_head)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoint(x0, n_layers):\n",
    "    xl = x0\n",
    "    for i in range(n_layers):\n",
    "        xl = auto_interacting(xl, d=6, n_attention_head=2)\n",
    "    \n",
    "    return xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_5), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_6), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_7), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_10), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(64, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_11), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_12), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_13), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_16), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_17), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_18), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_21), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(12, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_22), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_23), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_24), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_27), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_28), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_29), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_32), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(12, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "autoint_layer = build_autoint(embed_map, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoint_layer = tf.keras.layers.Flatten()(autoint_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 84) dtype=float32 (created by layer 'flatten')>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoint_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 448) dtype=float32 (created by layer 'flatten_1')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_embed_map = tf.keras.layers.Flatten()(embed_map)\n",
    "flattened_embed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tf.keras.layers.Dense(256,activation = 'relu',\n",
    "                name='mlp_dense0')(flattened_embed_map)\n",
    "\n",
    "#vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "vec = tf.keras.layers.Dense(128,\n",
    "                            activation = 'relu',\n",
    "                            name='mlp_dense1')(vec)\n",
    "\n",
    "#vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "instance = tf.keras.layers.Dense(64,\n",
    "                            activation = 'relu',\n",
    "                            name='instance')(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_instance = tf.keras.layers.Concatenate(axis=-1)([autoint_layer,instance]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 148) dtype=float32 (created by layer 'concatenate_4')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " userid (InputLayer)         [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " device (InputLayer)         [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " feedid (InputLayer)         [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " authorid (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " duration_level (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 64)                1280064   ['userid[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 1, 64)                192       ['device[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 1, 64)                6171456   ['feedid[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 1, 64)                1178176   ['authorid[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 1, 64)                384       ['duration_level[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 7, 64)                0         ['embedding[0][0]',           \n",
      "                                                                     'embedding_1[0][0]',         \n",
      "                                                                     'embedding_2[0][0]',         \n",
      "                                                                     'embedding_3[0][0]',         \n",
      "                                                                     'embedding_4[0][0]',         \n",
      "                                                                     'embedding_4[0][0]',         \n",
      "                                                                     'embedding_4[0][0]']         \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLa  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_6 (TFOpLa  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLamb  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose (TF  (None, 6, 7)                 0         ['tf.linalg.matmul_1[0][0]']  \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_5 (TFOpLa  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_1 (  (None, 6, 7)                 0         ['tf.linalg.matmul_6[0][0]']  \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_3 (TFOpLa  (None, 7, 7)                 0         ['tf.linalg.matmul[0][0]',    \n",
      " mbda)                                                               'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_8 (TFOpLa  (None, 7, 7)                 0         ['tf.linalg.matmul_5[0][0]',  \n",
      " mbda)                                                               'tf.compat.v1.transpose_1[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)  (None, 7, 7)                 0         ['tf.linalg.matmul_3[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_2 (TFOpLa  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.softmax_1 (TFOpLambd  (None, 7, 7)                 0         ['tf.linalg.matmul_8[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_7 (TFOpLa  (None, 7, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_4 (TFOpLa  (None, 7, 6)                 0         ['tf.nn.softmax[0][0]',       \n",
      " mbda)                                                               'tf.linalg.matmul_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_9 (TFOpLa  (None, 7, 6)                 0         ['tf.nn.softmax_1[0][0]',     \n",
      " mbda)                                                               'tf.linalg.matmul_7[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 7, 12)                0         ['tf.linalg.matmul_4[0][0]',  \n",
      " )                                                                   'tf.linalg.matmul_9[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_10 (TFOpL  (None, 7, 12)                0         ['concatenate[0][0]']         \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 7, 12)                0         ['concatenate_1[0][0]',       \n",
      " Lambda)                                                             'tf.linalg.matmul_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 7, 12)                0         ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_12 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_17 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_11 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_2 (  (None, 6, 7)                 0         ['tf.linalg.matmul_12[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_16 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_3 (  (None, 6, 7)                 0         ['tf.linalg.matmul_17[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_14 (TFOpL  (None, 7, 7)                 0         ['tf.linalg.matmul_11[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_2[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_19 (TFOpL  (None, 7, 7)                 0         ['tf.linalg.matmul_16[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_3[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax_2 (TFOpLambd  (None, 7, 7)                 0         ['tf.linalg.matmul_14[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_13 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.softmax_3 (TFOpLambd  (None, 7, 7)                 0         ['tf.linalg.matmul_19[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_18 (TFOpL  (None, 7, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_15 (TFOpL  (None, 7, 6)                 0         ['tf.nn.softmax_2[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_13[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_20 (TFOpL  (None, 7, 6)                 0         ['tf.nn.softmax_3[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_18[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 7, 12)                0         ['tf.linalg.matmul_15[0][0]', \n",
      " )                                                                   'tf.linalg.matmul_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_21 (TFOpL  (None, 7, 12)                0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 7, 12)                0         ['concatenate_2[0][0]',       \n",
      " OpLambda)                                                           'tf.linalg.matmul_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 7, 12)                0         ['tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_23 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_28 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_22 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_4 (  (None, 6, 7)                 0         ['tf.linalg.matmul_23[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_27 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_5 (  (None, 6, 7)                 0         ['tf.linalg.matmul_28[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_25 (TFOpL  (None, 7, 7)                 0         ['tf.linalg.matmul_22[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_4[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_30 (TFOpL  (None, 7, 7)                 0         ['tf.linalg.matmul_27[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_5[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax_4 (TFOpLambd  (None, 7, 7)                 0         ['tf.linalg.matmul_25[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_24 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.softmax_5 (TFOpLambd  (None, 7, 7)                 0         ['tf.linalg.matmul_30[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_29 (TFOpL  (None, 7, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_26 (TFOpL  (None, 7, 6)                 0         ['tf.nn.softmax_4[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_24[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_31 (TFOpL  (None, 7, 6)                 0         ['tf.nn.softmax_5[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_29[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 7, 12)                0         ['tf.linalg.matmul_26[0][0]', \n",
      " )                                                                   'tf.linalg.matmul_31[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_32 (TFOpL  (None, 7, 12)                0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 448)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 7, 12)                0         ['concatenate_3[0][0]',       \n",
      " OpLambda)                                                           'tf.linalg.matmul_32[0][0]'] \n",
      "                                                                                                  \n",
      " mlp_dense0 (Dense)          (None, 256)                  114944    ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 7, 12)                0         ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mlp_dense1 (Dense)          (None, 128)                  32896     ['mlp_dense0[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 84)                   0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " instance (Dense)            (None, 64)                   8256      ['mlp_dense1[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 148)                  0         ['flatten[0][0]',             \n",
      " )                                                                   'instance[0][0]']            \n",
      "                                                                                                  \n",
      " feedid1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " feedid2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " short_play (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    149       ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8786517 (33.52 MB)\n",
      "Trainable params: 8786517 (33.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "base_model = Model(sparse_inputs, final_instance)\n",
    "vec = base_model.outputs[0]\n",
    "\n",
    "final_output = tf.keras.layers.Dense(1,\"sigmoid\")(vec)\n",
    "model = Model(inputs=base_model.inputs, outputs=final_output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:33.613490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n",
      "2024-07-29 13:05:33.615888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c3:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:05:37.142605: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f4e68cb7c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 13:05:37.142624: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-07-29 13:05:37.146763: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 13:05:37.167306: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722229537.240459  223741 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2109/2109 [==============================] - 23s 9ms/step - loss: 1999.5020 - auc: 0.5504 - val_loss: 908.5131 - val_auc: 0.6057\n",
      "Epoch 2/20\n",
      "2109/2109 [==============================] - 19s 9ms/step - loss: 431.3531 - auc: 0.5968 - val_loss: 133.5247 - val_auc: 0.6169\n",
      "Epoch 3/20\n",
      "2109/2109 [==============================] - 19s 9ms/step - loss: 45.0798 - auc: 0.6009 - val_loss: 5.1030 - val_auc: 0.6189\n",
      "Epoch 4/20\n",
      "2109/2109 [==============================] - 19s 9ms/step - loss: 1.4630 - auc: 0.6036 - val_loss: 0.6436 - val_auc: 0.6240\n",
      "Epoch 5/20\n",
      " 283/2109 [===>..........................] - ETA: 12s - loss: 0.6582 - auc: 0.6056"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "# 20\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "\n",
    "\n",
    "hist = model.fit(train_sparse_x,\n",
    "                 train_label,\n",
    "                 epochs=20,\n",
    "                 batch_size=2048,\n",
    "                 shuffle=True,\n",
    "                 verbose=para['verbose'],\n",
    "                 validation_data=(val_sparse_x,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(para['callback']['base_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# infer\n",
    "\"\"\"\n",
    "auc: 0.73412085\n",
    "logloss: 0.57356197\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "model.load_weights(para['callback']['base_model'])\n",
    "y_test_pred = model.predict(test_sparse_x,batch_size=10000)\n",
    "auc_metric = tf.keras.metrics.AUC()\n",
    "auc_metric.update_state(y_test[0], y_test_pred)\n",
    "auc = auc_metric.result().numpy()\n",
    "\n",
    "logloss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "\n",
    "logloss_metric.update_state(y_test[0], y_test_pred)\n",
    "log_loss = logloss_metric.result().numpy()\n",
    "print('auc:',auc)\n",
    "print('logloss:',log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
