{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:56:45.737065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 19:56:45.795795: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-25 19:56:45.795819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-25 19:56:45.797559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-25 19:56:45.807019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 19:56:46.993986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2028/2475276552.py:15: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:56:48.092119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n",
      "2024-07-25 19:56:48.097424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "def seed_tensorflow(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_train_label\n",
    "# short_play\n",
    "# effective_play\n",
    "# long_play\n",
    "# complete_play\n",
    "select_label = 'effective_play'\n",
    "\n",
    "# base_model = \"./model/kuairand_backbone/mlp_{}.h5\".format(select_label)\n",
    "CPF_model = \"./model/wechat_CPF/mlp_{}.h5\".format(select_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = {'data_path':'./wechat_input/',\n",
    "        'embedding_dim':64,\n",
    "        'seed':0,\n",
    "        'lr':1e-5,\n",
    "        'batch_size':512,\n",
    "        'epochs':64,\n",
    "        'verbose':1,\n",
    "        'callback':{\n",
    "                     'monitor':'val_auc',\n",
    "                     'patience':10,\n",
    "                     'CPF_model':CPF_model\n",
    "                     },\n",
    "        'mlp_dims':[256,128,64],\n",
    "        'mlp_act':'relu',\n",
    "        'mlp_dps':[.5,.5,.5],\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = pd.read_csv(para['data_path']+\"action.csv\")\n",
    "feed_emb1 = np.load(para['data_path']+\"feedid_emb_64.npy\")\n",
    "feed_emb2 = np.load(para['data_path']+\"embeddings_array_wechat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>date_</th>\n",
       "      <th>device</th>\n",
       "      <th>read_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>play</th>\n",
       "      <th>stay</th>\n",
       "      <th>click_avatar</th>\n",
       "      <th>...</th>\n",
       "      <th>videoplayseconds</th>\n",
       "      <th>pcr</th>\n",
       "      <th>duration_level</th>\n",
       "      <th>threshold</th>\n",
       "      <th>binary_train_label</th>\n",
       "      <th>short_play</th>\n",
       "      <th>effective_play</th>\n",
       "      <th>long_play</th>\n",
       "      <th>complete_play</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>63138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>1533</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>43011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>1302</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>23367</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>25886</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210182</th>\n",
       "      <td>19999</td>\n",
       "      <td>62197</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6369</td>\n",
       "      <td>6566</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.219621</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210183</th>\n",
       "      <td>19999</td>\n",
       "      <td>44479</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13212</td>\n",
       "      <td>13708</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210184</th>\n",
       "      <td>19999</td>\n",
       "      <td>43644</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32215</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210185</th>\n",
       "      <td>19999</td>\n",
       "      <td>40090</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5618</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210186</th>\n",
       "      <td>19999</td>\n",
       "      <td>45995</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160560</td>\n",
       "      <td>161024</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7210187 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  feedid  date_  device  read_comment  comment  like    play  \\\n",
       "0             0   63138      1       1             0        0     0     250   \n",
       "1             0   43011      1       1             0        0     0     750   \n",
       "2             0   23367      1       1             0        0     0     250   \n",
       "3             0   25886      1       1             0        0     0       0   \n",
       "4             0     983      1       1             0        0     0     250   \n",
       "...         ...     ...    ...     ...           ...      ...   ...     ...   \n",
       "7210182   19999   62197     12       2             0        0     0    6369   \n",
       "7210183   19999   44479     12       2             0        0     0   13212   \n",
       "7210184   19999   43644     12       2             0        0     0       0   \n",
       "7210185   19999   40090     12       2             0        0     0       0   \n",
       "7210186   19999   45995     12       2             1        0     0  160560   \n",
       "\n",
       "           stay  click_avatar  ...  videoplayseconds       pcr  \\\n",
       "0          1533             0  ...                16  0.015625   \n",
       "1          1302             0  ...                31  0.024194   \n",
       "2           800             0  ...                12  0.020833   \n",
       "3          1496             0  ...                16  0.000000   \n",
       "4           976             0  ...                19  0.013158   \n",
       "...         ...           ...  ...               ...       ...   \n",
       "7210182    6566             0  ...                29  0.219621   \n",
       "7210183   13708             0  ...                11  1.000000   \n",
       "7210184   32215             0  ...                29  0.000000   \n",
       "7210185    5618             0  ...                12  0.000000   \n",
       "7210186  161024             0  ...                60  1.000000   \n",
       "\n",
       "         duration_level  threshold  binary_train_label  short_play  \\\n",
       "0                     1   0.648438                   0           1   \n",
       "1                     2   0.625000                   0           1   \n",
       "2                     0   0.656250                   0           1   \n",
       "3                     1   0.648438                   0           1   \n",
       "4                     1   0.648438                   0           1   \n",
       "...                 ...        ...                 ...         ...   \n",
       "7210182               2   0.625000                   0           1   \n",
       "7210183               0   0.656250                   1           0   \n",
       "7210184               2   0.625000                   0           1   \n",
       "7210185               0   0.656250                   0           1   \n",
       "7210186               4   0.562500                   1           0   \n",
       "\n",
       "         effective_play  long_play  complete_play  test_label  \n",
       "0                     0          0              0           0  \n",
       "1                     0          0              0           0  \n",
       "2                     0          0              0           0  \n",
       "3                     0          0              0           0  \n",
       "4                     0          0              0           0  \n",
       "...                 ...        ...            ...         ...  \n",
       "7210182               0          0              0           0  \n",
       "7210183               1          1              1           0  \n",
       "7210184               0          0              0           0  \n",
       "7210185               0          0              0           0  \n",
       "7210186               1          1              1           1  \n",
       "\n",
       "[7210187 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = action.sort_values(by=['date_'])\n",
    "\n",
    "train= action.groupby('userid').apply(lambda x: x[:int(len(x)*0.6)]).reset_index(drop=True)\n",
    "valid= action.groupby('userid').apply(lambda x: x[int(len(x)*0.6):int(len(x)*0.8)]).reset_index(drop=True)\n",
    "test= action.groupby('userid').apply(lambda x: x[int(len(x)*0.8):]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:18.029728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "def get_layer(shape,name,dtype='int32',d1=None,d2=None,pretrain=None,trainable=False):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=shape, name=name+'_input', dtype=dtype))\n",
    "    if d1 is None:\n",
    "        d1 = pretrain.shape[0]\n",
    "        d2 = pretrain.shape[1]\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=int(d1),\n",
    "                       output_dim=int(d2),\n",
    "                       weights=[pretrain] if pretrain is not None else None,\n",
    "                       trainable=trainable,\n",
    "                       name=name+'_embedding'))\n",
    "    return model\n",
    "\n",
    "def mlp_bias():\n",
    "    uid_lay = get_layer((1,),'uid',d1 = max(action['userid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    did_lay = get_layer((1,),'did',d1 = max(action['device'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "\n",
    "    vid_lay = get_layer((1,),'vid',d1 = max(action['feedid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    aid_lay = get_layer((1,),'aid',d1 = max(action['authorid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    pre_lay_1 = get_layer((1,),'pretrain1',pretrain = feed_emb1,trainable=False)\n",
    "\n",
    "    pre_lay_2 = get_layer((1,),'pretrain2',pretrain = feed_emb2,trainable=False)\n",
    "    duration_lay = get_layer((1,),'duration_id',d1 = max(action['duration_level'])+1, d2=para['embedding_dim'],trainable=True)\n",
    "    label_input = tf.keras.layers.Input(shape=(1,), name='labels')\n",
    "    Lay_bi =[uid_lay,did_lay,vid_lay,aid_lay,pre_lay_1,duration_lay,pre_lay_2]\n",
    "    lay_bi_outs = []\n",
    "    for l in Lay_bi:\n",
    "        lay_bi_outs += l.outputs\n",
    "\n",
    "    vec = tf.keras.layers.concatenate(lay_bi_outs,axis=-1)\n",
    "    vec = tf.squeeze(vec,axis=1)\n",
    "\n",
    "    vec = tf.keras.layers.Dense(256,\n",
    "                                    activation = 'relu',\n",
    "                                    name='mlp_dense0')(vec)\n",
    "\n",
    "    vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "    vec = tf.keras.layers.Dense(128,\n",
    "                                    activation = 'relu',\n",
    "                                    name='mlp_dense1')(vec)\n",
    "\n",
    "    vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "    instance = tf.keras.layers.Dense(64,\n",
    "                                    activation = 'relu',\n",
    "                                    name='instance')(vec)\n",
    "    #instance = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "    #instance = tf.keras.layers.BatchNormalization(name='instance')(vec)\n",
    "\n",
    "    model_inputs = []\n",
    "    for l in Lay_bi:\n",
    "        model_inputs += l.inputs\n",
    "    model_inputs += [label_input]\n",
    "    model = tf.keras.Model(inputs=model_inputs,outputs=[instance])\n",
    "\n",
    "\n",
    "    return model # 返回一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:18.056212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n",
      "2024-07-25 19:57:18.665798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " uid_input (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " did_input (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " vid_input (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " aid_input (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " pretrain1_input (InputLaye  [(None, 1)]                  0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " duration_id_input (InputLa  [(None, 1)]                  0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " pretrain2_input (InputLaye  [(None, 1)]                  0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " uid_embedding (Embedding)   (None, 1, 64)                1280000   ['uid_input[0][0]']           \n",
      "                                                                                                  \n",
      " did_embedding (Embedding)   (None, 1, 64)                192       ['did_input[0][0]']           \n",
      "                                                                                                  \n",
      " vid_embedding (Embedding)   (None, 1, 64)                6171392   ['vid_input[0][0]']           \n",
      "                                                                                                  \n",
      " aid_embedding (Embedding)   (None, 1, 64)                1178112   ['aid_input[0][0]']           \n",
      "                                                                                                  \n",
      " pretrain1_embedding (Embed  (None, 1, 64)                6171392   ['pretrain1_input[0][0]']     \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " duration_id_embedding (Emb  (None, 1, 64)                320       ['duration_id_input[0][0]']   \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " pretrain2_embedding (Embed  (None, 1, 64)                6171392   ['pretrain2_input[0][0]']     \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 1, 448)               0         ['uid_embedding[0][0]',       \n",
      "                                                                     'did_embedding[0][0]',       \n",
      "                                                                     'vid_embedding[0][0]',       \n",
      "                                                                     'aid_embedding[0][0]',       \n",
      "                                                                     'pretrain1_embedding[0][0]', \n",
      "                                                                     'duration_id_embedding[0][0]'\n",
      "                                                                    , 'pretrain2_embedding[0][0]']\n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOp  (None, 448)                  0         ['concatenate[0][0]']         \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " mlp_dense0 (Dense)          (None, 256)                  114944    ['tf.compat.v1.squeeze[0][0]']\n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['mlp_dense0[0][0]']          \n",
      "                                                                                                  \n",
      " mlp_dense1 (Dense)          (None, 128)                  32896     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['mlp_dense1[0][0]']          \n",
      "                                                                                                  \n",
      " instance (Dense)            (None, 64)                   8256      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " labels (InputLayer)         [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    65        ['instance[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21128961 (80.60 MB)\n",
      "Trainable params: 8786177 (33.52 MB)\n",
      "Non-trainable params: 12342784 (47.08 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "base_model_backbone = mlp_bias()\n",
    "\n",
    "\n",
    "# n*64\n",
    "vec = base_model_backbone.outputs[0] # instance     dim=64\n",
    "final_output = tf.keras.layers.Dense(1,\"sigmoid\")(vec)\n",
    "\n",
    "# Create a model that includes the routed output\n",
    "model = Model(inputs=base_model_backbone.inputs, outputs=final_output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:22.019028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "features = ['userid','device','feedid','authorid','feedid','duration_level','feedid',select_label]\n",
    "\n",
    "# # short_play binary_train_label effective_play long_play complete_play\n",
    "def get_input(df,is_test=False):\n",
    "    X = []\n",
    "    for f in features:\n",
    "        X.append(df[f].values.reshape(-1,1))\n",
    "    y = [df[select_label].values.reshape(-1,1)]\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = get_input(train,is_test=False)\n",
    "X_valid,y_valid = get_input(valid,is_test=False)\n",
    "X_test,y_test = get_input(test,is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_tensor = tf.random.normal([10, 64], mean=0.0, stddev=1.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prototypes(tf.keras.layers.Layer):\n",
    "    name = 'prototypes'\n",
    "    def __init__(self,\n",
    "                 k ,\n",
    "                 beta1=0.0,\n",
    "                 beta2=0.0,\n",
    "                 beta3=0.0,\n",
    "                 init_prototypes = None,\n",
    "                 **kwargs):\n",
    "        super(Prototypes, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.beta1, self.beta2, self.beta3 = beta1, beta2, beta3\n",
    "        # y_train[0]\n",
    "        self.init_prototypes = init_prototypes\n",
    "        self.binary_classifiers = []\n",
    "        # \n",
    "        for _ in range(5):\n",
    "            mlp = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            mlp.build((None, 64))\n",
    "            self.binary_classifiers.append(mlp)\n",
    "            \n",
    "    # define prototype\n",
    "    def build(self, input_shape):\n",
    "        self.prototypes = self.add_weight(\n",
    "            name='prototypes',\n",
    "            shape=(1, self.k, 64),\n",
    "            initializer=tf.keras.initializers.Constant(self.init_prototypes[tf.newaxis, :, :]),\n",
    "            trainable=True\n",
    "        )\n",
    "        super(Prototypes, self).build(input_shape)\n",
    "\n",
    "    \n",
    "    def orthogonality_loss(self,prototypes, k):\n",
    "        # 确保传入的 k 是偶数\n",
    "        assert k % 2 == 0, \"k must be even.\"\n",
    "        D = k // 2\n",
    "\n",
    "        prototypes = tf.reshape(prototypes, [k, 64])\n",
    "        prototypes = tf.nn.l2_normalize(prototypes, axis=1)\n",
    "\n",
    "        cosine_similarity_matrix = tf.matmul(prototypes, prototypes, transpose_b=True)\n",
    "\n",
    "        upper_triangular_part = tf.linalg.band_part(cosine_similarity_matrix, 0, -1)\n",
    "        upper_triangular_part -= tf.linalg.band_part(cosine_similarity_matrix, 0, 0)  # 去掉对角线\n",
    "\n",
    "        loss = tf.reduce_sum(tf.square(upper_triangular_part))\n",
    "        normalization_factor = D * (2 * D - 1)\n",
    "        loss /= normalization_factor\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def contrastive_loss(self, vec, labels, duration):\n",
    "        # Normalize the vectors\n",
    "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
    "        #print(\"vec_norm\",vec_norm)\n",
    "        sim_matrix = tf.matmul(vec_norm, vec_norm, transpose_b=True)\n",
    "        #print(\"sim_matrix\",sim_matrix)\n",
    "        # Create masks\n",
    "        label_eq = tf.equal(labels, tf.transpose(labels))\n",
    "        #print(\"label_eq\",label_eq)\n",
    "        duration_eq = tf.equal(duration, tf.transpose(duration))\n",
    "        #print(\"duration_eq\",duration_eq)\n",
    "        mask = tf.logical_and(label_eq, duration_eq)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        #print(\"mask\",mask)\n",
    "\n",
    "        # Compute the softmax denominator\n",
    "        exp_sim = tf.exp(sim_matrix)\n",
    "        total_sum = tf.reduce_sum(exp_sim)\n",
    "        #print(\"total_sum\",total_sum)\n",
    "        normal_sim = exp_sim / total_sum\n",
    "        #print(\"normal_sim\",normal_sim)\n",
    "\n",
    "        log_normal_sim = tf.math.log(normal_sim)\n",
    "        weighted_log_prob = mask * log_normal_sim\n",
    "\n",
    "        num_ones = tf.reduce_sum(mask)\n",
    "\n",
    "        loss = -tf.reduce_sum(weighted_log_prob)\n",
    "        return loss/num_ones\n",
    "\n",
    "    def assign_loss(self,output, vec):\n",
    "\n",
    "\n",
    "        output_norm = tf.nn.l2_normalize(output, axis=1)\n",
    "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
    "\n",
    "        cosine_similarity = tf.reduce_sum(tf.multiply(output_norm, vec_norm), axis=1)\n",
    "        loss = 1 - tf.reduce_mean(cosine_similarity)\n",
    "        return loss\n",
    "    def get_alpha(self,n_batch,duration_one_hot_expanded,alpha_product):\n",
    "        \"\"\"\n",
    "        alpha_product : (n,10,1)\n",
    "        duration_one_hot_expanded: (n,5,1)\n",
    "        \"\"\"\n",
    "        expanded_alpha = tf.reshape(alpha_product, [n_batch, 5, 2, 1])\n",
    "        masked = expanded_alpha * tf.cast(duration_one_hot_expanded, tf.float32)[:, :, :, tf.newaxis]\n",
    "        # n,2,1\n",
    "        alpha_para = tf.reduce_sum(masked, axis=1)\n",
    "        alpha_neg = alpha_para[:, 0, :]  # 第一个元素，保留最后一个轴\n",
    "        alpha_pos = alpha_para[:, 1, :]  # 第二个元素，保留最后一个轴\n",
    "        return alpha_neg,alpha_pos\n",
    "    def mse_loss(self,vec,label):\n",
    "        vec = tf.convert_to_tensor(vec, dtype=tf.float32)\n",
    "\n",
    "        label = tf.convert_to_tensor(label, dtype=tf.float32)\n",
    "\n",
    "        squeezed_label = tf.squeeze(label)\n",
    "\n",
    "        #tf.print(label, summarize=-1)\n",
    "        positive_indices = tf.where(squeezed_label == 1)\n",
    "        negative_indices = tf.where(squeezed_label == 0)\n",
    "\n",
    "        positive_samples = tf.gather(vec, positive_indices)\n",
    "        negative_samples = tf.gather(vec, negative_indices)\n",
    "        positive_samples = tf.squeeze(positive_samples,axis=1)\n",
    "        negative_samples = tf.squeeze(negative_samples,axis=1)\n",
    "        positive_loss = tf.reduce_mean(tf.square(positive_samples - self.pos_mean_prototype))\n",
    "        negative_loss = tf.reduce_mean(tf.square(negative_samples - self.neg_mean_prototype))\n",
    "\n",
    "        total_loss = positive_loss * 0.7 + negative_loss * 0.3\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def binary_loss(self,alpha, labels):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        loss = bce(labels, alpha)\n",
    "        return loss     \n",
    "    \n",
    "    def create_mask(self,duration, batch_size, num_classes=40, bucket_size=8):\n",
    "        start_indices = duration * bucket_size\n",
    "        masks = []\n",
    "        for i in range(batch_size):\n",
    "            mask = tf.concat([\n",
    "                tf.zeros((start_indices[i, 0], 1), dtype=tf.float32),\n",
    "                tf.ones((bucket_size, 1), dtype=tf.float32),\n",
    "                tf.zeros((num_classes - start_indices[i, 0] - bucket_size, 1), dtype=tf.float32)\n",
    "            ], axis=0)\n",
    "            masks.append(mask)\n",
    "        return tf.stack(masks)\n",
    "    def binary_crossentropy_manual(self, y_true, y_pred,para):\n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "        # 防止 log(0) 的情况发生\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "        weights = tf.where(y_true == 1, para, 1.0)\n",
    "        weighted_loss = loss * weights\n",
    "\n",
    "        positive_loss = tf.boolean_mask(weighted_loss, y_true == 1)\n",
    "        negative_loss = tf.boolean_mask(weighted_loss, y_true == 0)\n",
    "        #total_positive_loss = tf.reduce_sum(positive_loss)\n",
    "        #total_negative_loss = tf.reduce_sum(negative_loss)\n",
    "\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    def prototype_loss(self,prototypes_output):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        labels = tf.constant([[0], [1], [0], [1], [0], [1], [0], [1], [0], [1]], dtype=tf.float32)\n",
    "        loss = bce(labels, prototypes_output)\n",
    "        return loss\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "\n",
    "        vec,duration,label = x\n",
    "        a = tf.expand_dims(vec, -2)\n",
    "\n",
    "        b = self.prototypes\n",
    "        n_batch = tf.shape(vec)[0]\n",
    "\n",
    "        dot_product = tf.multiply(a, b)\n",
    "\n",
    "        norm_a = tf.norm(a, axis=-1, keepdims=True)  # 保持维度，使得维度是 [batch_size, 1, 1]\n",
    "\n",
    "        norm_b = tf.norm(b, axis=-1, keepdims=True)  # 保持维度，使得维度是 [1, k, 1]\n",
    "\n",
    "        # (batch_size, k,1)\n",
    "        sum_product = tf.reduce_sum(dot_product, axis=-1, keepdims=True)\n",
    "        cos_similarity = sum_product / (norm_a * norm_b)\n",
    "        \n",
    "        #tf.print(cos_similarity.shape)\n",
    "        \"\"\"\n",
    "        tf.print(vec,summarize=-1)\n",
    "        tf.print(self.prototypes,summarize=-1)\n",
    "        tf.print(\"Input duration\")\n",
    "        tf.print(duration.shape)\n",
    "        tf.print(\"Input label\")\n",
    "        tf.print(label.shape)\n",
    "        #tf.print(vec)\n",
    "        tf.print(duration, summarize=-1)\n",
    "        tf.print(label, summarize=-1)\n",
    "\n",
    "        tf.print(dot_product.shape)\n",
    "        tf.print(dot_product)\n",
    "\n",
    "        tf.print(sum_product.shape)\n",
    "        tf.print(sum_product,summarize=-1)\n",
    "\n",
    "        tf.print(norm_a.shape)\n",
    "        tf.print(norm_a)\n",
    "\n",
    "        tf.print(norm_b.shape)\n",
    "        tf.print(norm_b)\n",
    "\n",
    "        tf.print(cos_similarity, summarize=-1)\n",
    "        \"\"\"\n",
    "        # （batch_size, k//2, 2, 1)\n",
    "        reshaped_sum_product = tf.reshape(cos_similarity, (n_batch, self.k // 2, 2))\n",
    "\n",
    "        temperature = 0.05\n",
    "\n",
    "        scaled_logits = reshaped_sum_product / temperature\n",
    "\n",
    "        softmaxed_sum_product = tf.nn.softmax(scaled_logits, axis=2)\n",
    "\n",
    "        # (batch_size, k,1)\n",
    "        alpha_product = tf.reshape(softmaxed_sum_product, (n_batch, self.k, 1))\n",
    "        #tf.print(alpha_product,summarize=-1)\n",
    "        \"\"\"\n",
    "        tf.print(alpha_product.shape)\n",
    "        tf.print(alpha_product)\n",
    "        \"\"\"\n",
    "        #self.new = alpha_product\n",
    "        # n_batch, self.k, 64\n",
    "        # b:(1, k, 64)\n",
    "        alpha_product_expanded = tf.broadcast_to(alpha_product, [n_batch, self.k, 64])\n",
    "\n",
    "        # calculate\n",
    "        weighted_products = alpha_product_expanded * b\n",
    "\n",
    "        # add up\n",
    "        reshaped_weighted_products = tf.reshape(weighted_products, [n_batch, self.k//2, 2, 64])\n",
    "\n",
    "        # (batchsize,5,64)\n",
    "        pre_output = tf.reduce_sum(reshaped_weighted_products, axis=2)  # 对第三个维度求和\n",
    "\n",
    "        # 1 -> [0,1,0,0,0]\n",
    "        duration_one_hot = tf.one_hot(duration, depth=5)\n",
    "        duration_one_hot_expanded = tf.expand_dims(duration_one_hot, -1)\n",
    "        #(batch_size, 5, 1)\n",
    "        duration_one_hot_expanded = tf.squeeze(duration_one_hot_expanded, axis=[1])\n",
    "\n",
    "        masked_output = pre_output * duration_one_hot_expanded\n",
    "        # output (batchsize,64)\n",
    "        output = tf.reduce_sum(masked_output, axis=1)\n",
    "\n",
    "\n",
    "        # get_alpha\n",
    "        alpha_neg, alpha_pos = self.get_alpha(n_batch,duration_one_hot_expanded,alpha_product)\n",
    "        #tf.print(alpha_pos,summarize = -1)\n",
    "        #tf.print(alpha_neg,summarize = -1)\n",
    "        #tf.print(alpha_pos,summarize   =-1)\n",
    "        #tf.print(label, summarize=-1)\n",
    "        classifier_outputs = []\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(5):\n",
    "            classifier_output = self.binary_classifiers[i](tf.squeeze(self.prototypes,axis=0))\n",
    "             # (batch_size, 1, 1)#\n",
    "            #classifier_output += i\n",
    "            classifier_outputs.append(classifier_output)\n",
    "        \n",
    "        #tf.print(classifier_outputs,summarize = -1)\n",
    "        #tf.print(classifier_outputs.shape,summarize = -1)\n",
    "        indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "        prototype_output_elements = []\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            tensor_index = i // 2  \n",
    "            element = tf.gather(classifier_outputs[tensor_index], idx, axis=0) \n",
    "            prototype_output_elements.append(element)\n",
    "\n",
    "        prototype_output = tf.concat(prototype_output_elements, axis=0)\n",
    "        prototype_output = tf.expand_dims(prototype_output,axis=1)\n",
    "        #(1,10)\n",
    "  \n",
    "        positive_indices = 2 * duration + 1\n",
    "        negative_indices = 2 * duration\n",
    "\n",
    "        positive_indices = tf.clip_by_value(positive_indices, 0, prototype_output.shape[0] - 1)\n",
    "        negative_indices = tf.clip_by_value(negative_indices, 0, prototype_output.shape[0] - 1)\n",
    "        #tf.print(alpha_pos,summarize = -1)\n",
    "        # 提取对应的 prototype_output 值\n",
    "        positive_values = tf.gather(prototype_output, positive_indices)\n",
    "        negative_values = tf.gather(prototype_output, negative_indices)\n",
    "        #positive_values = tf.squeeze(positive_values,axis=0)\n",
    "        #negative_values = tf.squeeze(negative_values,axis=0)\n",
    "        output = alpha_pos * tf.squeeze(positive_values,axis=-1) \n",
    "        assign_loss = self.assign_loss(output,vec)\n",
    "\n",
    "        binary_loss = self.binary_crossentropy_manual(label,output,1.0)\n",
    "        #self.add_loss(1.0 * binary_loss)\n",
    "        # 5\n",
    "        assign = self.binary_crossentropy_manual(label,alpha_pos,1.0)\n",
    "        self.add_loss(0.1 * assign)\n",
    "\n",
    "        ortho_loss = self.orthogonality_loss(self.prototypes,self.k)\n",
    "        self.add_loss(0.3 * ortho_loss)\n",
    "\n",
    "        contrastive_loss = self.contrastive_loss(vec,label,duration)\n",
    "        self.add_loss(0.12 * contrastive_loss)\n",
    "\n",
    "        proto_loss = self.prototype_loss(prototype_output)\n",
    "        self.add_loss(0.1 * proto_loss)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:22.091401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n",
      "2024-07-25 19:57:26.266876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "backbone = mlp_bias()\n",
    "#backbone.load_weights(\"./model/kuairand_CPF/mlp_effect_backbone.h5\")\n",
    "\n",
    "duration_id_input_output = backbone.get_layer(\"duration_id_input\").output\n",
    "labels_output = backbone.get_layer(\"labels\").output\n",
    "\n",
    "flag = 1\n",
    "if flag:\n",
    "    prototypes_layer = Prototypes(k=10,init_prototypes = proto_tensor\n",
    "                                  )\n",
    "\n",
    "    output = prototypes_layer([backbone.outputs[0],duration_id_input_output,labels_output])\n",
    "    #output.shape\n",
    "else:\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(backbone.outputs[0])\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=backbone.inputs, outputs=[output])\n",
    "\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/wechat_CPF/mlp_effective_play.h5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para['callback']['CPF_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:26.300496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 19:57:31.990640: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f42c9ed6700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-25 19:57:31.990680: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-07-25 19:57:32.004485: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-25 19:57:32.048250: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721908652.176833    2559 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2109/2109 [==============================] - 107s 47ms/step - loss: 2.9157 - auc: 0.5240 - val_loss: 2.7391 - val_auc: 0.5580\n",
      "Epoch 2/100\n",
      "2109/2109 [==============================] - 47s 23ms/step - loss: 2.7140 - auc: 0.5423 - val_loss: 2.6384 - val_auc: 0.5940\n",
      "Epoch 3/100\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 2.6517 - auc: 0.5592 - val_loss: 2.6034 - val_auc: 0.6133\n",
      "Epoch 4/100\n",
      "2109/2109 [==============================] - 48s 23ms/step - loss: 2.6210 - auc: 0.5761 - val_loss: 2.5802 - val_auc: 0.6309\n",
      "Epoch 5/100\n",
      "2109/2109 [==============================] - 46s 22ms/step - loss: 2.6012 - auc: 0.5915 - val_loss: 2.5673 - val_auc: 0.6394\n",
      "Epoch 6/100\n",
      "2109/2109 [==============================] - 48s 23ms/step - loss: 2.5887 - auc: 0.6019 - val_loss: 2.5589 - val_auc: 0.6459\n",
      "Epoch 7/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.5787 - auc: 0.6132 - val_loss: 2.5494 - val_auc: 0.6575\n",
      "Epoch 8/100\n",
      "2109/2109 [==============================] - 45s 22ms/step - loss: 2.5673 - auc: 0.6311 - val_loss: 2.5353 - val_auc: 0.6783\n",
      "Epoch 9/100\n",
      "2109/2109 [==============================] - 47s 22ms/step - loss: 2.5477 - auc: 0.6635 - val_loss: 2.5072 - val_auc: 0.7133\n",
      "Epoch 10/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.5203 - auc: 0.7019 - val_loss: 2.4887 - val_auc: 0.7316\n",
      "Epoch 11/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.5038 - auc: 0.7211 - val_loss: 2.4839 - val_auc: 0.7354\n",
      "Epoch 12/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4953 - auc: 0.7298 - val_loss: 2.4815 - val_auc: 0.7375\n",
      "Epoch 13/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4895 - auc: 0.7356 - val_loss: 2.4794 - val_auc: 0.7392\n",
      "Epoch 14/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4853 - auc: 0.7395 - val_loss: 2.4776 - val_auc: 0.7405\n",
      "Epoch 15/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4816 - auc: 0.7431 - val_loss: 2.4764 - val_auc: 0.7415\n",
      "Epoch 16/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4785 - auc: 0.7460 - val_loss: 2.4752 - val_auc: 0.7423\n",
      "Epoch 17/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4758 - auc: 0.7485 - val_loss: 2.4745 - val_auc: 0.7430\n",
      "Epoch 18/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4736 - auc: 0.7505 - val_loss: 2.4739 - val_auc: 0.7434\n",
      "Epoch 19/100\n",
      "2109/2109 [==============================] - 50s 23ms/step - loss: 2.4719 - auc: 0.7520 - val_loss: 2.4733 - val_auc: 0.7438\n",
      "Epoch 20/100\n",
      "2109/2109 [==============================] - 46s 22ms/step - loss: 2.4698 - auc: 0.7538 - val_loss: 2.4728 - val_auc: 0.7442\n",
      "Epoch 21/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4683 - auc: 0.7551 - val_loss: 2.4725 - val_auc: 0.7444\n",
      "Epoch 22/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4668 - auc: 0.7565 - val_loss: 2.4725 - val_auc: 0.7445\n",
      "Epoch 23/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4654 - auc: 0.7578 - val_loss: 2.4723 - val_auc: 0.7446\n",
      "Epoch 24/100\n",
      "2109/2109 [==============================] - 46s 22ms/step - loss: 2.4642 - auc: 0.7587 - val_loss: 2.4722 - val_auc: 0.7447\n",
      "Epoch 25/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4632 - auc: 0.7595 - val_loss: 2.4722 - val_auc: 0.7447\n",
      "Epoch 26/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4620 - auc: 0.7606 - val_loss: 2.4723 - val_auc: 0.7446\n",
      "Epoch 27/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4610 - auc: 0.7614 - val_loss: 2.4725 - val_auc: 0.7445\n",
      "Epoch 28/100\n",
      "2109/2109 [==============================] - 42s 20ms/step - loss: 2.4602 - auc: 0.7620 - val_loss: 2.4722 - val_auc: 0.7445\n",
      "Epoch 29/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4593 - auc: 0.7627 - val_loss: 2.4727 - val_auc: 0.7443\n",
      "Epoch 30/100\n",
      "2109/2109 [==============================] - 46s 22ms/step - loss: 2.4586 - auc: 0.7633 - val_loss: 2.4728 - val_auc: 0.7442\n",
      "Epoch 31/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4578 - auc: 0.7639 - val_loss: 2.4730 - val_auc: 0.7440\n",
      "Epoch 32/100\n",
      "2109/2109 [==============================] - 44s 21ms/step - loss: 2.4571 - auc: 0.7645 - val_loss: 2.4732 - val_auc: 0.7439\n",
      "Epoch 33/100\n",
      "2109/2109 [==============================] - 45s 22ms/step - loss: 2.4565 - auc: 0.7649 - val_loss: 2.4734 - val_auc: 0.7437\n",
      "Epoch 34/100\n",
      "2109/2109 [==============================] - 45s 21ms/step - loss: 2.4559 - auc: 0.7653 - val_loss: 2.4736 - val_auc: 0.7435\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # 监控验证集损失\n",
    "    mode='min',          # 最小化验证集损失\n",
    "    patience=para['callback']['patience']  # 等待的轮数\n",
    ")\n",
    "\n",
    "checkpoint_auc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=para['callback']['CPF_model'],  # 保存模型的路径\n",
    "    monitor='val_auc',  # 监控验证集 AUC\n",
    "    mode='max',          # 保存验证 AUC 最大的模型\n",
    "    save_weights_only=True,  # 只保存模型权重\n",
    "    save_best_only=True  # 只保存最佳模型\n",
    ")\n",
    "\n",
    "hist = model.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs=100,\n",
    "                 batch_size=2048,\n",
    "                 shuffle=True,\n",
    "                 verbose=para['verbose'],\n",
    "                 callbacks=[checkpoint_auc,es_callback],\n",
    "                 validation_data=(X_valid,y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 20:24:08.992620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:17:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 1s 5ms/step\n",
      "auc: 0.73319656\n",
      "logloss: 0.5956694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# infer\n",
    "\"\"\"\n",
    "short\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "model.load_weights(para['callback']['CPF_model'])\n",
    "y_test_pred = model.predict(X_test,batch_size=10000)\n",
    "auc_metric = tf.keras.metrics.AUC()\n",
    "auc_metric.update_state(y_test[0], y_test_pred)\n",
    "auc = auc_metric.result().numpy()\n",
    "\n",
    "logloss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "\n",
    "logloss_metric.update_state(y_test[0], y_test_pred)\n",
    "log_loss = logloss_metric.result().numpy()\n",
    "print('auc:',auc)\n",
    "print('logloss:',log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
