{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:18.217853: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-28 22:04:18.243082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 22:04:18.243116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 22:04:18.243740: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 22:04:18.247949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 22:04:18.769849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_685084/4022545289.py:26: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:19.380554: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.406973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.407146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.455433: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.455570: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.455649: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.455707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-07-28 22:04:19.456388: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.456489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.456550: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.457692: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.457838: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.457917: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.458007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.458068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:19.458114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "def seed_tensorflow(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_label = 'effective_play'\n",
    "CPF_model = \"./model/kuairand_CPF/Autoint_{}.h5\".format(select_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = {'data_path':'./kuai_input/',\n",
    "        'embedding_dim':64,\n",
    "        'seed':0,\n",
    "        'lr':1e-5,\n",
    "        'batch_size':512,\n",
    "        'epochs':64,\n",
    "        'verbose':1,\n",
    "        'callback':{\n",
    "                     'monitor':'val_auc',\n",
    "                     'patience':10,\n",
    "                     'base_model':CPF_model\n",
    "                     },\n",
    "        'mlp_dims':[256,128,64],\n",
    "        'mlp_act':'relu',\n",
    "        'mlp_dps':[.5,.5,.5],\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = pd.read_csv(para['data_path']+\"action.csv\")\n",
    "feed_emb = np.load(para['data_path']+\"embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hourmin</th>\n",
       "      <th>time_ms</th>\n",
       "      <th>is_click</th>\n",
       "      <th>is_like</th>\n",
       "      <th>is_follow</th>\n",
       "      <th>is_comment</th>\n",
       "      <th>is_forward</th>\n",
       "      <th>...</th>\n",
       "      <th>outsite_share_all_cnt</th>\n",
       "      <th>pcr</th>\n",
       "      <th>duration_level</th>\n",
       "      <th>threshold</th>\n",
       "      <th>binary_train_label</th>\n",
       "      <th>short_play</th>\n",
       "      <th>effective_play</th>\n",
       "      <th>long_play</th>\n",
       "      <th>complete_play</th>\n",
       "      <th>test_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1399</td>\n",
       "      <td>20220411</td>\n",
       "      <td>1900</td>\n",
       "      <td>1649675512388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.902597</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>4</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6857</td>\n",
       "      <td>20220416</td>\n",
       "      <td>2000</td>\n",
       "      <td>1650111976017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.041237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5590</td>\n",
       "      <td>20220420</td>\n",
       "      <td>1600</td>\n",
       "      <td>1650444367095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.868750</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>4</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5899</td>\n",
       "      <td>20220411</td>\n",
       "      <td>1100</td>\n",
       "      <td>1649645295928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3376</td>\n",
       "      <td>20220411</td>\n",
       "      <td>1100</td>\n",
       "      <td>1649648827559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>154.725146</td>\n",
       "      <td>0.024707</td>\n",
       "      <td>2</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266970</th>\n",
       "      <td>25491</td>\n",
       "      <td>1362</td>\n",
       "      <td>20220502</td>\n",
       "      <td>1100</td>\n",
       "      <td>1651461216855</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.206897</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>3</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266971</th>\n",
       "      <td>25491</td>\n",
       "      <td>5690</td>\n",
       "      <td>20220502</td>\n",
       "      <td>1600</td>\n",
       "      <td>1651478750711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.254658</td>\n",
       "      <td>0.064382</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266972</th>\n",
       "      <td>25491</td>\n",
       "      <td>5690</td>\n",
       "      <td>20220502</td>\n",
       "      <td>1600</td>\n",
       "      <td>1651478750711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.254658</td>\n",
       "      <td>0.064382</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266973</th>\n",
       "      <td>25491</td>\n",
       "      <td>3014</td>\n",
       "      <td>20220507</td>\n",
       "      <td>1800</td>\n",
       "      <td>1651918814747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266974</th>\n",
       "      <td>25491</td>\n",
       "      <td>1555</td>\n",
       "      <td>20220508</td>\n",
       "      <td>700</td>\n",
       "      <td>1651964016379</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.517857</td>\n",
       "      <td>0.985446</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1266975 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id      date  hourmin        time_ms  is_click  \\\n",
       "0              0      1399  20220411     1900  1649675512388         0   \n",
       "1              0      6857  20220416     2000  1650111976017         0   \n",
       "2              0      5590  20220420     1600  1650444367095         0   \n",
       "3              1      5899  20220411     1100  1649645295928         0   \n",
       "4              1      3376  20220411     1100  1649648827559         0   \n",
       "...          ...       ...       ...      ...            ...       ...   \n",
       "1266970    25491      1362  20220502     1100  1651461216855         0   \n",
       "1266971    25491      5690  20220502     1600  1651478750711         0   \n",
       "1266972    25491      5690  20220502     1600  1651478750711         0   \n",
       "1266973    25491      3014  20220507     1800  1651918814747         1   \n",
       "1266974    25491      1555  20220508      700  1651964016379         1   \n",
       "\n",
       "         is_like  is_follow  is_comment  is_forward  ...  \\\n",
       "0              0          0           0           0  ...   \n",
       "1              0          0           0           0  ...   \n",
       "2              0          0           0           0  ...   \n",
       "3              0          0           0           0  ...   \n",
       "4              0          0           0           0  ...   \n",
       "...          ...        ...         ...         ...  ...   \n",
       "1266970        0          0           0           0  ...   \n",
       "1266971        0          0           0           0  ...   \n",
       "1266972        0          0           0           0  ...   \n",
       "1266973        0          0           0           0  ...   \n",
       "1266974        0          0           0           0  ...   \n",
       "\n",
       "         outsite_share_all_cnt       pcr  duration_level  threshold  \\\n",
       "0                     3.902597  0.006598               4   0.335938   \n",
       "1                     6.041237  0.000000               2   0.421875   \n",
       "2                     4.868750  0.008224               4   0.335938   \n",
       "3                     0.805882  0.000000               4   0.335938   \n",
       "4                   154.725146  0.024707               2   0.421875   \n",
       "...                        ...       ...             ...        ...   \n",
       "1266970              11.206897  0.010102               3   0.386719   \n",
       "1266971              35.254658  0.064382               1   0.492188   \n",
       "1266972              35.254658  0.064382               1   0.492188   \n",
       "1266973               1.873563  1.000000               1   0.492188   \n",
       "1266974             165.517857  0.985446               1   0.492188   \n",
       "\n",
       "         binary_train_label  short_play  effective_play  long_play  \\\n",
       "0                         0           1               0          0   \n",
       "1                         0           1               0          0   \n",
       "2                         0           1               0          0   \n",
       "3                         0           1               0          0   \n",
       "4                         0           1               0          0   \n",
       "...                     ...         ...             ...        ...   \n",
       "1266970                   0           1               0          0   \n",
       "1266971                   0           1               0          0   \n",
       "1266972                   0           1               0          0   \n",
       "1266973                   1           0               1          1   \n",
       "1266974                   1           0               1          1   \n",
       "\n",
       "         complete_play test_label  \n",
       "0                    0          0  \n",
       "1                    0          0  \n",
       "2                    0          0  \n",
       "3                    0          0  \n",
       "4                    0          0  \n",
       "...                ...        ...  \n",
       "1266970              0          0  \n",
       "1266971              0          0  \n",
       "1266972              0          0  \n",
       "1266973              1          0  \n",
       "1266974              0          0  \n",
       "\n",
       "[1266975 rows x 120 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = action[['date','user_id','video_id','duration_level','tab','user_active_degree','author_id','music_id',\n",
    "                 'short_play', 'binary_train_label', 'effective_play',\n",
    "                 'long_play', 'complete_play']]\n",
    "categories_to_keep = ['full_active', 'high_active', 'middle_active','low_active']\n",
    "\n",
    "action = action[action['user_active_degree'].isin(categories_to_keep)]\n",
    "\n",
    "category_mapping = {\n",
    "    'low_active': 0,\n",
    "    'middle_active': 1,\n",
    "    'high_active': 2,\n",
    "    'full_active': 3\n",
    "}\n",
    "\n",
    "action['user_active_degree_encoded'] = action['user_active_degree'].map(category_mapping)\n",
    "action['music_id_encoded'], _ = pd.factorize(action['music_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= action.groupby('user_id').apply(lambda x: x[:int(len(x)*0.6)]).reset_index(drop=True)\n",
    "valid= action.groupby('user_id').apply(lambda x: x[int(len(x)*0.6):int(len(x)*0.8)]).reset_index(drop=True)\n",
    "test= action.groupby('user_id').apply(lambda x: x[int(len(x)*0.8):]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>duration_level</th>\n",
       "      <th>tab</th>\n",
       "      <th>user_active_degree</th>\n",
       "      <th>author_id</th>\n",
       "      <th>music_id</th>\n",
       "      <th>short_play</th>\n",
       "      <th>binary_train_label</th>\n",
       "      <th>effective_play</th>\n",
       "      <th>long_play</th>\n",
       "      <th>complete_play</th>\n",
       "      <th>user_active_degree_encoded</th>\n",
       "      <th>music_id_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220411</td>\n",
       "      <td>0</td>\n",
       "      <td>1399</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>full_active</td>\n",
       "      <td>5799</td>\n",
       "      <td>9154013497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220416</td>\n",
       "      <td>0</td>\n",
       "      <td>6857</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>full_active</td>\n",
       "      <td>5412</td>\n",
       "      <td>9145692691</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220420</td>\n",
       "      <td>0</td>\n",
       "      <td>5590</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>full_active</td>\n",
       "      <td>4571</td>\n",
       "      <td>9156647291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220411</td>\n",
       "      <td>0</td>\n",
       "      <td>6606</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>full_active</td>\n",
       "      <td>2881</td>\n",
       "      <td>9141209460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220411</td>\n",
       "      <td>0</td>\n",
       "      <td>6606</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>full_active</td>\n",
       "      <td>2881</td>\n",
       "      <td>9141209460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732586</th>\n",
       "      <td>20220417</td>\n",
       "      <td>25491</td>\n",
       "      <td>3336</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>high_active</td>\n",
       "      <td>4503</td>\n",
       "      <td>9156857725</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732587</th>\n",
       "      <td>20220410</td>\n",
       "      <td>25491</td>\n",
       "      <td>6606</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>high_active</td>\n",
       "      <td>2881</td>\n",
       "      <td>9141209460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732588</th>\n",
       "      <td>20220428</td>\n",
       "      <td>25491</td>\n",
       "      <td>1799</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>high_active</td>\n",
       "      <td>2462</td>\n",
       "      <td>9155771466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732589</th>\n",
       "      <td>20220428</td>\n",
       "      <td>25491</td>\n",
       "      <td>1799</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>high_active</td>\n",
       "      <td>2462</td>\n",
       "      <td>9155771466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732590</th>\n",
       "      <td>20220430</td>\n",
       "      <td>25491</td>\n",
       "      <td>2426</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>high_active</td>\n",
       "      <td>1381</td>\n",
       "      <td>9167347422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732591 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  user_id  video_id  duration_level  tab user_active_degree  \\\n",
       "0       20220411        0      1399               4    1        full_active   \n",
       "1       20220416        0      6857               2    0        full_active   \n",
       "2       20220420        0      5590               4    1        full_active   \n",
       "3       20220411        0      6606               4    1        full_active   \n",
       "4       20220411        0      6606               4    0        full_active   \n",
       "...          ...      ...       ...             ...  ...                ...   \n",
       "732586  20220417    25491      3336               4    2        high_active   \n",
       "732587  20220410    25491      6606               4    2        high_active   \n",
       "732588  20220428    25491      1799               2    6        high_active   \n",
       "732589  20220428    25491      1799               2    6        high_active   \n",
       "732590  20220430    25491      2426               0    2        high_active   \n",
       "\n",
       "        author_id    music_id  short_play  binary_train_label  effective_play  \\\n",
       "0            5799  9154013497           1                   0               0   \n",
       "1            5412  9145692691           1                   0               0   \n",
       "2            4571  9156647291           1                   0               0   \n",
       "3            2881  9141209460           1                   0               0   \n",
       "4            2881  9141209460           1                   0               0   \n",
       "...           ...         ...         ...                 ...             ...   \n",
       "732586       4503  9156857725           1                   0               0   \n",
       "732587       2881  9141209460           1                   0               0   \n",
       "732588       2462  9155771466           1                   0               0   \n",
       "732589       2462  9155771466           1                   0               0   \n",
       "732590       1381  9167347422           0                   1               1   \n",
       "\n",
       "        long_play  complete_play  user_active_degree_encoded  music_id_encoded  \n",
       "0               0              0                           3                 0  \n",
       "1               0              0                           3                 1  \n",
       "2               0              0                           3                 2  \n",
       "3               0              0                           3               643  \n",
       "4               0              0                           3               643  \n",
       "...           ...            ...                         ...               ...  \n",
       "732586          0              0                           2              2156  \n",
       "732587          0              0                           2               643  \n",
       "732588          0              0                           2               398  \n",
       "732589          0              0                           2               398  \n",
       "732590          1              1                           2              1069  \n",
       "\n",
       "[732591 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:35.019620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.019816: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.019886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.019987: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.020052: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.020103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\"\"\"\n",
    "    uid_lay = get_layer((1,),'uid',d1 = max(action['user_id'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    videoid_lay = get_layer((1,),'video_id',d1 = max(action['video_id'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "    duration_lay = get_layer((1,),'duration_id',d1 = max(action['duration_level'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "    tab_lay = get_layer((1,),'tab',d1 = max(action['tab'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "    user_active_degree_encoded = get_layer((1,),'user_active_degree_encoded',d1 = max(action['user_active_degree_encoded'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "    author_lay = get_layer((1,),'author_id',d1 = max(action['author_id'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "    music_lay = get_layer((1,),'music_id_encoded',d1 = max(action['music_id_encoded'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
    "\"\"\"\n",
    "\n",
    "features = ['user_id','video_id','duration_level','tab','user_active_degree_encoded','author_id','music_id_encoded','video_id',select_label]\n",
    "\n",
    "# # short_play binary_train_label effective_play long_play complete_play\n",
    "def get_input(df,is_test=False):\n",
    "    X = []\n",
    "    for f in features:\n",
    "        X.append(df[f].values.reshape(-1,1))\n",
    "    y = [df[select_label].values.reshape(-1,1)]\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = get_input(train,is_test=False)\n",
    "X_valid,y_valid = get_input(valid,is_test=False)\n",
    "X_test,y_test = get_input(test,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  1399     4 ...     0  1399     0]\n",
      " [    0  6857     2 ...     1  6857     0]\n",
      " [    0  5590     4 ...     2  5590     0]\n",
      " ...\n",
      " [25491  1799     2 ...   398  1799     0]\n",
      " [25491  1799     2 ...   398  1799     0]\n",
      " [25491  2426     0 ...  1069  2426     1]]\n"
     ]
    }
   ],
   "source": [
    "train_X = np.hstack(X_train)\n",
    "test_X = np.hstack(X_test)\n",
    "valid_X = np.hstack(X_valid)\n",
    "\n",
    "# 打印重构的数组以验证\n",
    "print(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = y_train[0].flatten()\n",
    "test_y = y_test[0].flatten()\n",
    "valid_y = y_valid[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(732591, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feats = ['user_id','video_id','duration_level','tab','user_active_degree_encoded','author_id','music_id_encoded','video_id',select_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_x = [train[f].values for f in sparse_feats]\n",
    "val_sparse_x = [valid[f].values for f in sparse_feats]\n",
    "test_sparse_x = [test[f].values for f in sparse_feats]\n",
    "\n",
    "train_label = [train[select_label].values]\n",
    "val_label = [valid[select_label].values]\n",
    "test_label = [test[select_label].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0,     0,     0, ..., 25491, 25491, 25491]),\n",
       " array([1399, 6857, 5590, ..., 1799, 1799, 2426]),\n",
       " array([4, 2, 4, ..., 2, 2, 0]),\n",
       " array([1, 0, 1, ..., 6, 6, 2]),\n",
       " array([3, 3, 3, ..., 2, 2, 2]),\n",
       " array([5799, 5412, 4571, ..., 2462, 2462, 1381]),\n",
       " array([   0,    1,    2, ...,  398,  398, 1069]),\n",
       " array([1399, 6857, 5590, ..., 1799, 1799, 2426]),\n",
       " array([0, 0, 0, ..., 0, 0, 1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_inputs = []\n",
    "\n",
    "input0 = tf.keras.layers.Input([1], name='user_id',dtype=tf.int32)\n",
    "input1 = tf.keras.layers.Input([1], name='video_id',dtype=tf.int32)\n",
    "input2 = tf.keras.layers.Input([1], name='duration_level',dtype=tf.int32)\n",
    "input3 = tf.keras.layers.Input([1], name='tab',dtype=tf.int32)\n",
    "input4 = tf.keras.layers.Input([1], name='user_active_degree_encoded',dtype=tf.int32)\n",
    "input5 = tf.keras.layers.Input([1], name='author_id',dtype=tf.int32)\n",
    "input6 = tf.keras.layers.Input([1], name='music_id_encoded',dtype=tf.int32)\n",
    "input7 = tf.keras.layers.Input([1], name='video_id1',dtype=tf.int32)\n",
    "input8 = tf.keras.layers.Input([1], name=select_label,dtype=tf.int32)\n",
    "\n",
    "sparse_inputs.append(input0)\n",
    "sparse_inputs.append(input1)\n",
    "sparse_inputs.append(input2)\n",
    "\n",
    "sparse_inputs.append(input3)\n",
    "sparse_inputs.append(input4)\n",
    "sparse_inputs.append(input5)\n",
    "\n",
    "sparse_inputs.append(input6)\n",
    "sparse_inputs.append(input7)\n",
    "sparse_inputs.append(input8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'video_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'duration_level')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'tab')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_active_degree_encoded')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'author_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'music_id_encoded')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'video_id1')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'effective_play')>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:35.185278: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.185486: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.185554: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.185652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.185714: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.185764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "sparse_kd_embed = []\n",
    "for i, _input in enumerate(sparse_inputs):\n",
    "    if i == 7:\n",
    "        tf.keras.layers.Embedding(input_dim=int(feed_emb.shape[0]),\n",
    "                       output_dim=int(feed_emb.shape[1]),\n",
    "                       weights=[feed_emb],\n",
    "                       trainable=False,\n",
    "                       name='pre_embedding')\n",
    "        sparse_kd_embed.append(_embed)\n",
    "    elif i==8:\n",
    "        continue\n",
    "    else:\n",
    "        f = sparse_feats[i]\n",
    "        voc_size = action[f].nunique()\n",
    "        _embed = tf.keras.layers.Embedding(voc_size+1, 64, embeddings_regularizer=tf.keras.regularizers.l2(0.5))(_input)\n",
    "        sparse_kd_embed.append(_embed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_1')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_2')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_3')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_4')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_5')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_6')>,\n",
       " <KerasTensor: shape=(None, 1, 64) dtype=float32 (created by layer 'embedding_6')>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_kd_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = sparse_kd_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_map = tf.keras.layers.Concatenate(axis=1)(input_embeds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_interacting(embed_map, d=6, n_attention_head=2):\n",
    "    \"\"\"\n",
    "    实现单层 AutoInt Interacting Layer\n",
    "    @param embed_map: 输入的embedding feature map, (?, n_feats, n_dim)\n",
    "    @param d: Q,K,V映射后的维度\n",
    "    @param n_attention_head: multi-head attention的个数\n",
    "    \"\"\"\n",
    "    assert len(embed_map.shape) == 3, \"Input embedding feature map must be 3-D tensor.\"\n",
    "    \n",
    "    k = embed_map.shape[-1]\n",
    "    \n",
    "    # 存储多个self-attention的结果\n",
    "    attention_heads = []\n",
    "    W_Q = []\n",
    "    W_K = []\n",
    "    W_V = []\n",
    "    \n",
    "    # 1.构建多个attention\n",
    "    for i in range(n_attention_head):\n",
    "        # 初始化W_Q, W_K, W_V\n",
    "        W_Q.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"query_\"+str(i)))  # k, d\n",
    "        W_K.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"key_\"+str(i)))  # k, d\n",
    "        W_V.append(tf.Variable(tf.random.truncated_normal(shape=(k, d)), name=\"value_\"+str(i)))  # k, d\n",
    "     \n",
    "    for i in range(n_attention_head):\n",
    "        # 映射到d维空间\n",
    "        embed_q = tf.matmul(embed_map, W_Q[i])  # ?, 39, d\n",
    "        embed_k = tf.matmul(embed_map, W_K[i])  # ?, 39, d\n",
    "        embed_v = tf.matmul(embed_map, W_V[i])  # ?, 39, d\n",
    "    \n",
    "        # 计算attention\n",
    "        energy = tf.matmul(embed_q, tf.transpose(embed_k, [0, 2, 1]))  # ?, 39, 39\n",
    "        attention = tf.nn.softmax(energy)  # ?, 39, 39\n",
    "    \n",
    "        attention_output = tf.matmul(attention, embed_v)  # ?, 39, d\n",
    "        attention_heads.append(attention_output)\n",
    "    \n",
    "    # 2.concat multi head\n",
    "    multi_attention_output = tf.keras.layers.Concatenate(axis=-1)(attention_heads)  # ?, 39, n_attention_head*d\n",
    "    \n",
    "    # 3.ResNet\n",
    "    w_res = tf.Variable(tf.random.truncated_normal(shape=(k, d*n_attention_head)), name=\"w_res_\"+str(i))  # k, d*n_attention_head\n",
    "    output = tf.keras.layers.Activation(\"relu\")(multi_attention_output + tf.matmul(embed_map, w_res))  # ?, 39, d*n_attention_head)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoint(x0, n_layers):\n",
    "    xl = x0\n",
    "    for i in range(n_layers):\n",
    "        xl = auto_interacting(xl, d=6, n_attention_head=2)\n",
    "    \n",
    "    return xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 64) dtype=float32 (created by layer 'concatenate')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_5), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_6), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_7), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(64, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_10), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(64, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_11), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_12), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_13), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_16), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_17), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_18), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_21), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(12, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_22), but are not present in its tracked objects:   <tf.Variable 'query_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_23), but are not present in its tracked objects:   <tf.Variable 'key_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_24), but are not present in its tracked objects:   <tf.Variable 'value_0:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_27), but are not present in its tracked objects:   <tf.Variable 'query_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_28), but are not present in its tracked objects:   <tf.Variable 'key_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_29), but are not present in its tracked objects:   <tf.Variable 'value_1:0' shape=(12, 6) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_32), but are not present in its tracked objects:   <tf.Variable 'w_res_1:0' shape=(12, 12) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "autoint_layer = build_autoint(embed_map, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 64) dtype=float32 (created by layer 'concatenate')>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoint_layer = tf.keras.layers.Flatten()(autoint_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 96) dtype=float32 (created by layer 'flatten')>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoint_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 512) dtype=float32 (created by layer 'flatten_1')>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_embed_map = tf.keras.layers.Flatten()(embed_map)\n",
    "flattened_embed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tf.keras.layers.Dense(256,activation = 'relu',\n",
    "                name='mlp_dense0')(flattened_embed_map)\n",
    "\n",
    "vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "vec = tf.keras.layers.Dense(128,\n",
    "                            activation = 'relu',\n",
    "                            name='mlp_dense1')(vec)\n",
    "\n",
    "vec = tf.keras.layers.Dropout(0.5)(vec)\n",
    "\n",
    "instance = tf.keras.layers.Dense(64,\n",
    "                            activation = 'relu',\n",
    "                            name='instance')(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 64) dtype=float32 (created by layer 'instance')>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_instance = tf.keras.layers.Concatenate(axis=-1)([autoint_layer,instance]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 160) dtype=float32 (created by layer 'concatenate_4')>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " user_id (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " video_id (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " duration_level (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tab (InputLayer)            [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " user_active_degree_encoded  [(None, 1)]                  0         []                            \n",
      "  (InputLayer)                                                                                    \n",
      "                                                                                                  \n",
      " author_id (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " music_id_encoded (InputLay  [(None, 1)]                  0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 64)                1592128   ['user_id[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 1, 64)                448896    ['video_id[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 1, 64)                384       ['duration_level[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 1, 64)                1024      ['tab[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 1, 64)                320       ['user_active_degree_encoded[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, 1, 64)                386048    ['author_id[0][0]']           \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)     (None, 1, 64)                439808    ['music_id_encoded[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8, 64)                0         ['embedding[0][0]',           \n",
      "                                                                     'embedding_1[0][0]',         \n",
      "                                                                     'embedding_2[0][0]',         \n",
      "                                                                     'embedding_3[0][0]',         \n",
      "                                                                     'embedding_4[0][0]',         \n",
      "                                                                     'embedding_5[0][0]',         \n",
      "                                                                     'embedding_6[0][0]',         \n",
      "                                                                     'embedding_6[0][0]']         \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLa  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_6 (TFOpLa  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLamb  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose (TF  (None, 6, 8)                 0         ['tf.linalg.matmul_1[0][0]']  \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_5 (TFOpLa  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_1 (  (None, 6, 8)                 0         ['tf.linalg.matmul_6[0][0]']  \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_3 (TFOpLa  (None, 8, 8)                 0         ['tf.linalg.matmul[0][0]',    \n",
      " mbda)                                                               'tf.compat.v1.transpose[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_8 (TFOpLa  (None, 8, 8)                 0         ['tf.linalg.matmul_5[0][0]',  \n",
      " mbda)                                                               'tf.compat.v1.transpose_1[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)  (None, 8, 8)                 0         ['tf.linalg.matmul_3[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_2 (TFOpLa  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.softmax_1 (TFOpLambd  (None, 8, 8)                 0         ['tf.linalg.matmul_8[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_7 (TFOpLa  (None, 8, 6)                 0         ['concatenate[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_4 (TFOpLa  (None, 8, 6)                 0         ['tf.nn.softmax[0][0]',       \n",
      " mbda)                                                               'tf.linalg.matmul_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_9 (TFOpLa  (None, 8, 6)                 0         ['tf.nn.softmax_1[0][0]',     \n",
      " mbda)                                                               'tf.linalg.matmul_7[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 8, 12)                0         ['tf.linalg.matmul_4[0][0]',  \n",
      " )                                                                   'tf.linalg.matmul_9[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_10 (TFOpL  (None, 8, 12)                0         ['concatenate[0][0]']         \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 8, 12)                0         ['concatenate_1[0][0]',       \n",
      " Lambda)                                                             'tf.linalg.matmul_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 8, 12)                0         ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_12 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_17 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_11 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_2 (  (None, 6, 8)                 0         ['tf.linalg.matmul_12[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_16 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_3 (  (None, 6, 8)                 0         ['tf.linalg.matmul_17[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_14 (TFOpL  (None, 8, 8)                 0         ['tf.linalg.matmul_11[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_2[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_19 (TFOpL  (None, 8, 8)                 0         ['tf.linalg.matmul_16[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_3[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax_2 (TFOpLambd  (None, 8, 8)                 0         ['tf.linalg.matmul_14[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_13 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.softmax_3 (TFOpLambd  (None, 8, 8)                 0         ['tf.linalg.matmul_19[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_18 (TFOpL  (None, 8, 6)                 0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_15 (TFOpL  (None, 8, 6)                 0         ['tf.nn.softmax_2[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_13[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_20 (TFOpL  (None, 8, 6)                 0         ['tf.nn.softmax_3[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_18[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 8, 12)                0         ['tf.linalg.matmul_15[0][0]', \n",
      " )                                                                   'tf.linalg.matmul_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_21 (TFOpL  (None, 8, 12)                0         ['activation[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 8, 12)                0         ['concatenate_2[0][0]',       \n",
      " OpLambda)                                                           'tf.linalg.matmul_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 8, 12)                0         ['tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_23 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_28 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_22 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_4 (  (None, 6, 8)                 0         ['tf.linalg.matmul_23[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_27 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_5 (  (None, 6, 8)                 0         ['tf.linalg.matmul_28[0][0]'] \n",
      " TFOpLambda)                                                                                      \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_25 (TFOpL  (None, 8, 8)                 0         ['tf.linalg.matmul_22[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_4[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_30 (TFOpL  (None, 8, 8)                 0         ['tf.linalg.matmul_27[0][0]', \n",
      " ambda)                                                              'tf.compat.v1.transpose_5[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax_4 (TFOpLambd  (None, 8, 8)                 0         ['tf.linalg.matmul_25[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_24 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.softmax_5 (TFOpLambd  (None, 8, 8)                 0         ['tf.linalg.matmul_30[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_29 (TFOpL  (None, 8, 6)                 0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 512)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_26 (TFOpL  (None, 8, 6)                 0         ['tf.nn.softmax_4[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_24[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_31 (TFOpL  (None, 8, 6)                 0         ['tf.nn.softmax_5[0][0]',     \n",
      " ambda)                                                              'tf.linalg.matmul_29[0][0]'] \n",
      "                                                                                                  \n",
      " mlp_dense0 (Dense)          (None, 256)                  131328    ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 8, 12)                0         ['tf.linalg.matmul_26[0][0]', \n",
      " )                                                                   'tf.linalg.matmul_31[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_32 (TFOpL  (None, 8, 12)                0         ['activation_1[0][0]']        \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['mlp_dense0[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 8, 12)                0         ['concatenate_3[0][0]',       \n",
      " OpLambda)                                                           'tf.linalg.matmul_32[0][0]'] \n",
      "                                                                                                  \n",
      " mlp_dense1 (Dense)          (None, 128)                  32896     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 8, 12)                0         ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['mlp_dense1[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 96)                   0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " instance (Dense)            (None, 64)                   8256      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 160)                  0         ['flatten[0][0]',             \n",
      " )                                                                   'instance[0][0]']            \n",
      "                                                                                                  \n",
      " video_id1 (InputLayer)      [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " effective_play (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    161       ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3041249 (11.60 MB)\n",
      "Trainable params: 3041249 (11.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = Model(sparse_inputs, final_instance)\n",
    "vec = base_model.outputs[0]\n",
    "\n",
    "final_output = tf.keras.layers.Dense(1,\"sigmoid\")(vec)\n",
    "model = Model(inputs=base_model.inputs, outputs=final_output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prototypes(tf.keras.layers.Layer):\n",
    "    name = 'prototypes'\n",
    "    def __init__(self,\n",
    "                 k ,\n",
    "                 beta1=0.0,\n",
    "                 beta2=0.0,\n",
    "                 beta3=0.0,\n",
    "                 init_prototypes = None,\n",
    "                 **kwargs):\n",
    "        super(Prototypes, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.beta1, self.beta2, self.beta3 = beta1, beta2, beta3\n",
    "        # y_train[0]\n",
    "        self.init_prototypes = init_prototypes\n",
    "        self.binary_classifiers = []\n",
    "        # \n",
    "        for _ in range(5):\n",
    "            mlp = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            mlp.build((None, 160))\n",
    "            self.binary_classifiers.append(mlp)\n",
    "            \n",
    "    # define prototype\n",
    "    def build(self, input_shape):\n",
    "        self.prototypes = self.add_weight(\n",
    "            name='prototypes',\n",
    "            shape=(1, self.k, 160),\n",
    "            initializer=tf.keras.initializers.Constant(self.init_prototypes[tf.newaxis, :, :]),\n",
    "            trainable=True\n",
    "        )\n",
    "        super(Prototypes, self).build(input_shape)\n",
    "\n",
    "    \n",
    "    def orthogonality_loss(self,prototypes, k):\n",
    "        # 确保传入的 k 是偶数\n",
    "        assert k % 2 == 0, \"k must be even.\"\n",
    "        D = k // 2\n",
    "\n",
    "        prototypes = tf.reshape(prototypes, [k, 160])\n",
    "        prototypes = tf.nn.l2_normalize(prototypes, axis=1)\n",
    "\n",
    "        cosine_similarity_matrix = tf.matmul(prototypes, prototypes, transpose_b=True)\n",
    "\n",
    "        upper_triangular_part = tf.linalg.band_part(cosine_similarity_matrix, 0, -1)\n",
    "        upper_triangular_part -= tf.linalg.band_part(cosine_similarity_matrix, 0, 0)  # 去掉对角线\n",
    "\n",
    "        loss = tf.reduce_sum(tf.square(upper_triangular_part))\n",
    "        normalization_factor = D * (2 * D - 1)\n",
    "        loss /= normalization_factor\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def contrastive_loss(self, vec, labels, duration):\n",
    "        # Normalize the vectors\n",
    "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
    "        #print(\"vec_norm\",vec_norm)\n",
    "        sim_matrix = tf.matmul(vec_norm, vec_norm, transpose_b=True)\n",
    "        #print(\"sim_matrix\",sim_matrix)\n",
    "        # Create masks\n",
    "        label_eq = tf.equal(labels, tf.transpose(labels))\n",
    "        #print(\"label_eq\",label_eq)\n",
    "        duration_eq = tf.equal(duration, tf.transpose(duration))\n",
    "        #print(\"duration_eq\",duration_eq)\n",
    "        mask = tf.logical_and(label_eq, duration_eq)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        #print(\"mask\",mask)\n",
    "\n",
    "        # Compute the softmax denominator\n",
    "        exp_sim = tf.exp(sim_matrix)\n",
    "        total_sum = tf.reduce_sum(exp_sim)\n",
    "        #print(\"total_sum\",total_sum)\n",
    "        normal_sim = exp_sim / total_sum\n",
    "        #print(\"normal_sim\",normal_sim)\n",
    "\n",
    "        log_normal_sim = tf.math.log(normal_sim)\n",
    "        weighted_log_prob = mask * log_normal_sim\n",
    "\n",
    "        num_ones = tf.reduce_sum(mask)\n",
    "\n",
    "        loss = -tf.reduce_sum(weighted_log_prob)\n",
    "        return loss/num_ones\n",
    "\n",
    "    def assign_loss(self,output, vec):\n",
    "\n",
    "\n",
    "        output_norm = tf.nn.l2_normalize(output, axis=1)\n",
    "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
    "\n",
    "        cosine_similarity = tf.reduce_sum(tf.multiply(output_norm, vec_norm), axis=1)\n",
    "        loss = 1 - tf.reduce_mean(cosine_similarity)\n",
    "        return loss\n",
    "    def get_alpha(self,n_batch,duration_one_hot_expanded,alpha_product):\n",
    "        \"\"\"\n",
    "        alpha_product : (n,10,1)\n",
    "        duration_one_hot_expanded: (n,5,1)\n",
    "        \"\"\"\n",
    "        expanded_alpha = tf.reshape(alpha_product, [n_batch, 5, 2, 1])\n",
    "        masked = expanded_alpha * tf.cast(duration_one_hot_expanded, tf.float32)[:, :, :, tf.newaxis]\n",
    "        # n,2,1\n",
    "        alpha_para = tf.reduce_sum(masked, axis=1)\n",
    "        alpha_neg = alpha_para[:, 0, :]  # 第一个元素，保留最后一个轴\n",
    "        alpha_pos = alpha_para[:, 1, :]  # 第二个元素，保留最后一个轴\n",
    "        return alpha_neg,alpha_pos\n",
    "    def mse_loss(self,vec,label):\n",
    "        vec = tf.convert_to_tensor(vec, dtype=tf.float32)\n",
    "\n",
    "        label = tf.convert_to_tensor(label, dtype=tf.float32)\n",
    "\n",
    "        squeezed_label = tf.squeeze(label)\n",
    "\n",
    "        #tf.print(label, summarize=-1)\n",
    "        positive_indices = tf.where(squeezed_label == 1)\n",
    "        negative_indices = tf.where(squeezed_label == 0)\n",
    "\n",
    "        positive_samples = tf.gather(vec, positive_indices)\n",
    "        negative_samples = tf.gather(vec, negative_indices)\n",
    "        positive_samples = tf.squeeze(positive_samples,axis=1)\n",
    "        negative_samples = tf.squeeze(negative_samples,axis=1)\n",
    "        positive_loss = tf.reduce_mean(tf.square(positive_samples - self.pos_mean_prototype))\n",
    "        negative_loss = tf.reduce_mean(tf.square(negative_samples - self.neg_mean_prototype))\n",
    "\n",
    "        total_loss = positive_loss * 0.7 + negative_loss * 0.3\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def binary_loss(self,y_true, y_pred):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        loss = bce(y_true, y_pred)\n",
    "        return loss     \n",
    "    \n",
    "    def create_mask(self,duration, batch_size, num_classes=40, bucket_size=8):\n",
    "        start_indices = duration * bucket_size\n",
    "        masks = []\n",
    "        for i in range(batch_size):\n",
    "            mask = tf.concat([\n",
    "                tf.zeros((start_indices[i, 0], 1), dtype=tf.float32),\n",
    "                tf.ones((bucket_size, 1), dtype=tf.float32),\n",
    "                tf.zeros((num_classes - start_indices[i, 0] - bucket_size, 1), dtype=tf.float32)\n",
    "            ], axis=0)\n",
    "            masks.append(mask)\n",
    "        return tf.stack(masks)\n",
    "    def binary_crossentropy_manual(self, y_true, y_pred,para):\n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "        # 防止 log(0) 的情况发生\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "        weights = tf.where(y_true == 1, para, 1.0)\n",
    "        weighted_loss = loss * weights\n",
    "\n",
    "        positive_loss = tf.boolean_mask(weighted_loss, y_true == 1)\n",
    "        negative_loss = tf.boolean_mask(weighted_loss, y_true == 0)\n",
    "        #total_positive_loss = tf.reduce_sum(positive_loss)\n",
    "        #total_negative_loss = tf.reduce_sum(negative_loss)\n",
    "\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    def prototype_loss(self,prototypes_output):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        labels = tf.constant([[0], [1], [0], [1], [0], [1], [0], [1], [0], [1]], dtype=tf.float32)\n",
    "        loss = bce(labels, prototypes_output)\n",
    "        return loss\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "\n",
    "        vec,duration,label = x\n",
    "        a = tf.expand_dims(vec, -2)\n",
    "\n",
    "        b = self.prototypes\n",
    "        n_batch = tf.shape(vec)[0]\n",
    "\n",
    "        dot_product = tf.multiply(a, b)\n",
    "\n",
    "        norm_a = tf.norm(a, axis=-1, keepdims=True)  # 保持维度，使得维度是 [batch_size, 1, 1]\n",
    "\n",
    "        norm_b = tf.norm(b, axis=-1, keepdims=True)  # 保持维度，使得维度是 [1, k, 1]\n",
    "\n",
    "        # (batch_size, k,1)\n",
    "        sum_product = tf.reduce_sum(dot_product, axis=-1, keepdims=True)\n",
    "        cos_similarity = sum_product / (norm_a * norm_b)\n",
    "        \n",
    "        #tf.print(cos_similarity.shape)\n",
    "        \"\"\"\n",
    "        tf.print(vec,summarize=-1)\n",
    "        tf.print(self.prototypes,summarize=-1)\n",
    "        tf.print(\"Input duration\")\n",
    "        tf.print(duration.shape)\n",
    "        tf.print(\"Input label\")\n",
    "        tf.print(label.shape)\n",
    "        #tf.print(vec)\n",
    "        tf.print(duration, summarize=-1)\n",
    "        tf.print(label, summarize=-1)\n",
    "\n",
    "        tf.print(dot_product.shape)\n",
    "        tf.print(dot_product)\n",
    "\n",
    "        tf.print(sum_product.shape)\n",
    "        tf.print(sum_product,summarize=-1)\n",
    "\n",
    "        tf.print(norm_a.shape)\n",
    "        tf.print(norm_a)\n",
    "\n",
    "        tf.print(norm_b.shape)\n",
    "        tf.print(norm_b)\n",
    "\n",
    "        tf.print(cos_similarity, summarize=-1)\n",
    "        \"\"\"\n",
    "        # （batch_size, k//2, 2, 1)\n",
    "        reshaped_sum_product = tf.reshape(cos_similarity, (n_batch, self.k // 2, 2))\n",
    "\n",
    "        temperature = 0.05\n",
    "\n",
    "        scaled_logits = reshaped_sum_product / temperature\n",
    "\n",
    "        softmaxed_sum_product = tf.nn.softmax(scaled_logits, axis=2)\n",
    "\n",
    "        # (batch_size, k,1)\n",
    "        alpha_product = tf.reshape(softmaxed_sum_product, (n_batch, self.k, 1))\n",
    "        #tf.print(alpha_product,summarize=-1)\n",
    "        \"\"\"\n",
    "        tf.print(alpha_product.shape)\n",
    "        tf.print(alpha_product)\n",
    "        \"\"\"\n",
    "        #self.new = alpha_product\n",
    "        # n_batch, self.k, 64\n",
    "        # b:(1, k, 64)\n",
    "        alpha_product_expanded = tf.broadcast_to(alpha_product, [n_batch, self.k, 160])\n",
    "\n",
    "        # calculate\n",
    "        weighted_products = alpha_product_expanded * b\n",
    "\n",
    "        # add up\n",
    "        reshaped_weighted_products = tf.reshape(weighted_products, [n_batch, self.k//2, 2, 160])\n",
    "\n",
    "        # (batchsize,5,64)\n",
    "        pre_output = tf.reduce_sum(reshaped_weighted_products, axis=2)  # 对第三个维度求和\n",
    "\n",
    "        # 1 -> [0,1,0,0,0]\n",
    "        duration_one_hot = tf.one_hot(duration, depth=5)\n",
    "        duration_one_hot_expanded = tf.expand_dims(duration_one_hot, -1)\n",
    "        #(batch_size, 5, 1)\n",
    "        duration_one_hot_expanded = tf.squeeze(duration_one_hot_expanded, axis=[1])\n",
    "\n",
    "        masked_output = pre_output * duration_one_hot_expanded\n",
    "        # output (batchsize,64)\n",
    "        output = tf.reduce_sum(masked_output, axis=1)\n",
    "\n",
    "\n",
    "        # get_alpha\n",
    "        alpha_neg, alpha_pos = self.get_alpha(n_batch,duration_one_hot_expanded,alpha_product)\n",
    "        #tf.print(alpha_pos,summarize = -1)\n",
    "        #tf.print(alpha_neg,summarize = -1)\n",
    "        #tf.print(alpha_pos,summarize   =-1)\n",
    "        #tf.print(label, summarize=-1)\n",
    "        classifier_outputs = []\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(5):\n",
    "            classifier_output = self.binary_classifiers[i](tf.squeeze(self.prototypes,axis=0))\n",
    "             # (batch_size, 1, 1)#\n",
    "            #classifier_output += i\n",
    "            classifier_outputs.append(classifier_output)\n",
    "        \n",
    "        #tf.print(classifier_outputs,summarize = -1)\n",
    "        #tf.print(classifier_outputs.shape,summarize = -1)\n",
    "        indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "        prototype_output_elements = []\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            tensor_index = i // 2  \n",
    "            element = tf.gather(classifier_outputs[tensor_index], idx, axis=0) \n",
    "            prototype_output_elements.append(element)\n",
    "\n",
    "        prototype_output = tf.concat(prototype_output_elements, axis=0)\n",
    "        prototype_output = tf.expand_dims(prototype_output,axis=1)\n",
    "        #(1,10)\n",
    "  \n",
    "        positive_indices = 2 * duration + 1\n",
    "        negative_indices = 2 * duration\n",
    "\n",
    "        positive_indices = tf.clip_by_value(positive_indices, 0, prototype_output.shape[0] - 1)\n",
    "        negative_indices = tf.clip_by_value(negative_indices, 0, prototype_output.shape[0] - 1)\n",
    "        #tf.print(alpha_pos,summarize = -1)\n",
    "        # 提取对应的 prototype_output 值\n",
    "        positive_values = tf.gather(prototype_output, positive_indices)\n",
    "        negative_values = tf.gather(prototype_output, negative_indices)\n",
    "        #positive_values = tf.squeeze(positive_values,axis=0)\n",
    "        #negative_values = tf.squeeze(negative_values,axis=0)\n",
    "        output = alpha_pos * tf.squeeze(positive_values,axis=-1) \n",
    "        assign_loss = self.assign_loss(output,vec)\n",
    "\n",
    "        #binary_loss = self.binary_crossentropy_manual(label,output,1.0)\n",
    "        #self.add_loss(1.0 * binary_loss)\n",
    "        # 5\n",
    "        assign = self.binary_loss(label,alpha_pos)\n",
    "        self.add_loss(0.1 * assign)\n",
    "\n",
    "        ortho_loss = self.orthogonality_loss(self.prototypes,self.k)\n",
    "        self.add_loss(0.3 * ortho_loss)\n",
    "\n",
    "        contrastive_loss = self.contrastive_loss(vec,label,duration)\n",
    "        self.add_loss(0.12 * contrastive_loss)\n",
    "\n",
    "        proto_loss = self.prototype_loss(prototype_output)\n",
    "        self.add_loss(0.1 * proto_loss)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_tensor = tf.random.normal([10, 160], mean=0.0, stddev=1.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:35.796312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.796661: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.796983: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.797103: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.797171: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:35.797230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:36.201898: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.202166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.202254: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.202373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.202451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.202510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "#backbone.load_weights(\"./model/kuairand_CPF/mlp_effect_backbone.h5\")\n",
    "\n",
    "duration_id_input_output = base_model.get_layer(\"duration_level\").output\n",
    "labels_output = base_model.get_layer(select_label).output\n",
    "\n",
    "flag = 1\n",
    "if flag:\n",
    "    prototypes_layer = Prototypes(k=10,init_prototypes = proto_tensor\n",
    "                                  )\n",
    "\n",
    "    output = prototypes_layer([base_model.outputs[0],duration_id_input_output,labels_output])\n",
    "    #output.shape\n",
    "else:\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(base_model.outputs[0])\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.inputs, outputs=[output])\n",
    "\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:04:36.220190: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.220427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.220531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.220641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.220737: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:04:36.220801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-07-28 22:04:39.297881: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f2c31e919a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-28 22:04:39.297910: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2024-07-28 22:04:39.302023: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-28 22:04:39.312794: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722175479.356764  685184 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431/1431 [==============================] - 15s 8ms/step - loss: 796.9949 - auc: 0.6070 - val_loss: 488.1787 - val_auc: 0.6536\n",
      "Epoch 2/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 306.5351 - auc: 0.6785 - val_loss: 168.6973 - val_auc: 0.7013\n",
      "Epoch 3/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 94.6712 - auc: 0.7110 - val_loss: 42.5994 - val_auc: 0.7245\n",
      "Epoch 4/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 20.4139 - auc: 0.7293 - val_loss: 7.1669 - val_auc: 0.7421\n",
      "Epoch 5/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 3.7320 - auc: 0.7428 - val_loss: 2.2403 - val_auc: 0.7536\n",
      "Epoch 6/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.1131 - auc: 0.7511 - val_loss: 2.0588 - val_auc: 0.7583\n",
      "Epoch 7/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0645 - auc: 0.7563 - val_loss: 2.0444 - val_auc: 0.7620\n",
      "Epoch 8/200\n",
      "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0507 - auc: 0.7610 - val_loss: 2.0345 - val_auc: 0.7641\n",
      "Epoch 9/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0414 - auc: 0.7647 - val_loss: 2.0279 - val_auc: 0.7664\n",
      "Epoch 10/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0340 - auc: 0.7684 - val_loss: 2.0222 - val_auc: 0.7682\n",
      "Epoch 11/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0292 - auc: 0.7718 - val_loss: 2.0196 - val_auc: 0.7705\n",
      "Epoch 12/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0263 - auc: 0.7744 - val_loss: 2.0184 - val_auc: 0.7729\n",
      "Epoch 13/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0238 - auc: 0.7774 - val_loss: 2.0160 - val_auc: 0.7754\n",
      "Epoch 14/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0212 - auc: 0.7808 - val_loss: 2.0145 - val_auc: 0.7771\n",
      "Epoch 15/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0186 - auc: 0.7841 - val_loss: 2.0120 - val_auc: 0.7798\n",
      "Epoch 16/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0158 - auc: 0.7873 - val_loss: 2.0097 - val_auc: 0.7822\n",
      "Epoch 17/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0133 - auc: 0.7897 - val_loss: 2.0081 - val_auc: 0.7838\n",
      "Epoch 18/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0113 - auc: 0.7918 - val_loss: 2.0063 - val_auc: 0.7859\n",
      "Epoch 19/200\n",
      "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0094 - auc: 0.7933 - val_loss: 2.0065 - val_auc: 0.7858\n",
      "Epoch 20/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 2.0080 - auc: 0.7943 - val_loss: 2.0049 - val_auc: 0.7867\n",
      "Epoch 21/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0063 - auc: 0.7956 - val_loss: 2.0034 - val_auc: 0.7881\n",
      "Epoch 22/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0053 - auc: 0.7961 - val_loss: 2.0034 - val_auc: 0.7876\n",
      "Epoch 23/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0040 - auc: 0.7970 - val_loss: 2.0015 - val_auc: 0.7887\n",
      "Epoch 24/200\n",
      "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0029 - auc: 0.7976 - val_loss: 2.0015 - val_auc: 0.7890\n",
      "Epoch 25/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0022 - auc: 0.7981 - val_loss: 2.0004 - val_auc: 0.7893\n",
      "Epoch 26/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0010 - auc: 0.7989 - val_loss: 2.0000 - val_auc: 0.7885\n",
      "Epoch 27/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0003 - auc: 0.7992 - val_loss: 1.9994 - val_auc: 0.7892\n",
      "Epoch 28/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 1.9994 - auc: 0.7997 - val_loss: 1.9984 - val_auc: 0.7904\n",
      "Epoch 29/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9984 - auc: 0.8002 - val_loss: 1.9989 - val_auc: 0.7902\n",
      "Epoch 30/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9976 - auc: 0.8006 - val_loss: 1.9967 - val_auc: 0.7905\n",
      "Epoch 31/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9969 - auc: 0.8010 - val_loss: 1.9966 - val_auc: 0.7903\n",
      "Epoch 32/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9960 - auc: 0.8016 - val_loss: 1.9960 - val_auc: 0.7908\n",
      "Epoch 33/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9956 - auc: 0.8019 - val_loss: 1.9980 - val_auc: 0.7905\n",
      "Epoch 34/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9950 - auc: 0.8025 - val_loss: 1.9960 - val_auc: 0.7912\n",
      "Epoch 35/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9943 - auc: 0.8028 - val_loss: 1.9952 - val_auc: 0.7913\n",
      "Epoch 36/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9937 - auc: 0.8033 - val_loss: 1.9957 - val_auc: 0.7913\n",
      "Epoch 37/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9932 - auc: 0.8037 - val_loss: 1.9953 - val_auc: 0.7907\n",
      "Epoch 38/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9927 - auc: 0.8042 - val_loss: 1.9950 - val_auc: 0.7911\n",
      "Epoch 39/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9922 - auc: 0.8048 - val_loss: 1.9938 - val_auc: 0.7919\n",
      "Epoch 40/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9916 - auc: 0.8051 - val_loss: 1.9942 - val_auc: 0.7918\n",
      "Epoch 41/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9909 - auc: 0.8057 - val_loss: 1.9936 - val_auc: 0.7924\n",
      "Epoch 42/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9907 - auc: 0.8060 - val_loss: 1.9945 - val_auc: 0.7913\n",
      "Epoch 43/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9899 - auc: 0.8068 - val_loss: 1.9940 - val_auc: 0.7918\n",
      "Epoch 44/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9897 - auc: 0.8071 - val_loss: 1.9932 - val_auc: 0.7924\n",
      "Epoch 45/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9888 - auc: 0.8077 - val_loss: 1.9940 - val_auc: 0.7917\n",
      "Epoch 46/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9885 - auc: 0.8081 - val_loss: 1.9929 - val_auc: 0.7928\n",
      "Epoch 47/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9879 - auc: 0.8088 - val_loss: 1.9934 - val_auc: 0.7926\n",
      "Epoch 48/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9873 - auc: 0.8093 - val_loss: 1.9934 - val_auc: 0.7933\n",
      "Epoch 49/200\n",
      "1431/1431 [==============================] - 10s 7ms/step - loss: 1.9870 - auc: 0.8097 - val_loss: 1.9937 - val_auc: 0.7923\n",
      "Epoch 50/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9864 - auc: 0.8102 - val_loss: 1.9931 - val_auc: 0.7926\n",
      "Epoch 51/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9854 - auc: 0.8105 - val_loss: 1.9921 - val_auc: 0.7931\n",
      "Epoch 52/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9848 - auc: 0.8110 - val_loss: 1.9917 - val_auc: 0.7935\n",
      "Epoch 53/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9841 - auc: 0.8115 - val_loss: 1.9921 - val_auc: 0.7939\n",
      "Epoch 54/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9840 - auc: 0.8120 - val_loss: 1.9926 - val_auc: 0.7928\n",
      "Epoch 55/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9835 - auc: 0.8123 - val_loss: 1.9922 - val_auc: 0.7933\n",
      "Epoch 56/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9830 - auc: 0.8128 - val_loss: 1.9920 - val_auc: 0.7935\n",
      "Epoch 57/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9826 - auc: 0.8134 - val_loss: 1.9913 - val_auc: 0.7942\n",
      "Epoch 58/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9823 - auc: 0.8134 - val_loss: 1.9919 - val_auc: 0.7937\n",
      "Epoch 59/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9813 - auc: 0.8144 - val_loss: 1.9926 - val_auc: 0.7935\n",
      "Epoch 60/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9805 - auc: 0.8150 - val_loss: 1.9927 - val_auc: 0.7934\n",
      "Epoch 61/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9803 - auc: 0.8152 - val_loss: 1.9917 - val_auc: 0.7936\n",
      "Epoch 62/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9798 - auc: 0.8158 - val_loss: 1.9915 - val_auc: 0.7941\n",
      "Epoch 63/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 1.9794 - auc: 0.8161 - val_loss: 1.9922 - val_auc: 0.7943\n",
      "Epoch 64/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9786 - auc: 0.8168 - val_loss: 1.9924 - val_auc: 0.7933\n",
      "Epoch 65/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9784 - auc: 0.8172 - val_loss: 1.9927 - val_auc: 0.7947\n",
      "Epoch 66/200\n",
      "1431/1431 [==============================] - 11s 8ms/step - loss: 1.9776 - auc: 0.8175 - val_loss: 1.9931 - val_auc: 0.7928\n",
      "Epoch 67/200\n",
      "1431/1431 [==============================] - 11s 7ms/step - loss: 1.9769 - auc: 0.8179 - val_loss: 1.9928 - val_auc: 0.7929\n"
     ]
    }
   ],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # 监控验证集损失\n",
    "    mode='min',          # 最小化验证集损失\n",
    "    patience=para['callback']['patience']  # 等待的轮数\n",
    ")\n",
    "\n",
    "checkpoint_auc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=para['callback']['base_model'],  # 保存模型的路径\n",
    "    monitor='val_auc',  # 监控验证集 AUC\n",
    "    mode='max',          # 保存验证 AUC 最大的模型\n",
    "    save_weights_only=True,  # 只保存模型权重\n",
    "    save_best_only=True  # 只保存最佳模型\n",
    ")\n",
    "\n",
    "\n",
    "hist = model.fit(train_sparse_x,\n",
    "                 train_label,\n",
    "                 epochs=200,\n",
    "                 batch_size=512,\n",
    "                 shuffle=True,\n",
    "                 verbose=para['verbose'],\n",
    "                 callbacks=[checkpoint_auc,es_callback],\n",
    "                 validation_data=(val_sparse_x,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:16:35.217087: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:16:35.217410: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:16:35.217495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:16:35.217596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:16:35.217663: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-28 22:16:35.217718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step\n",
      "auc: 0.78792197\n",
      "logloss: 0.47106457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# infer\n",
    "\"\"\"\n",
    "0.78959733\n",
    "0.46864867\n",
    "\"\"\"\n",
    "\n",
    "seed_tensorflow(seed=para['seed'])\n",
    "model.load_weights(para['callback']['base_model'])\n",
    "y_test_pred = model.predict(X_test,batch_size=10000)\n",
    "auc_metric = tf.keras.metrics.AUC()\n",
    "auc_metric.update_state(y_test[0], y_test_pred)\n",
    "auc = auc_metric.result().numpy()\n",
    "\n",
    "logloss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "\n",
    "logloss_metric.update_state(y_test[0], y_test_pred)\n",
    "log_loss = logloss_metric.result().numpy()\n",
    "print('auc:',auc)\n",
    "print('logloss:',log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
