{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwECuOJQOkVB",
        "outputId": "669332e7-3091-4fdf-e7cf-0a851a80143a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:20:49.420112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-28 14:20:49.444740: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-28 14:20:49.444768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-28 14:20:49.445359: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-28 14:20:49.449651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-28 14:20:49.914077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_590385/4022545289.py:26: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "True\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:20:50.456644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.478843: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.478985: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.519546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.519676: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.519749: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.519803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-28 14:20:50.521053: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.521176: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.521243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.521657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.522089: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.522217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.522330: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.522411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:20:50.522577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "print(tf.test.is_gpu_available())\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "def seed_tensorflow(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(config=session_conf)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# binary_train_label\n",
        "# short_play\n",
        "# effective_play\n",
        "# long_play\n",
        "# complete_play\n",
        "select_label = 'effective_play'\n",
        "\n",
        "# base_model = \"./model/kuairand_backbone/mlp_{}.h5\".format(select_label)\n",
        "CPF_model = \"./model/kuairand_CPF/DCN_{}.h5\".format(select_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HR60fa-TOkVD"
      },
      "outputs": [],
      "source": [
        "para = {'data_path':'./kuai_input/',\n",
        "        'embedding_dim':64,\n",
        "        'seed':0,\n",
        "        'lr':1e-5,\n",
        "        'batch_size':512,\n",
        "        'epochs':64,\n",
        "        'verbose':1,\n",
        "        'callback':{\n",
        "                     'monitor':'val_auc',\n",
        "                     'patience':10,\n",
        "                     'CPF_model':CPF_model\n",
        "                     },\n",
        "        'mlp_dims':[256,128,64],\n",
        "        'mlp_act':'relu',\n",
        "        'mlp_dps':[.5,.5,.5],\n",
        "       }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pTEhArDxOkVE"
      },
      "outputs": [],
      "source": [
        "action = pd.read_csv(para['data_path']+\"action.csv\")\n",
        "feed_emb = np.load(para['data_path']+\"embeddings.npy\")\n",
        "action = action[['user_id','video_id','duration_level','tab','user_active_degree','author_id','music_id',\n",
        "                 'short_play', 'binary_train_label', 'effective_play',\n",
        "                 'long_play', 'complete_play']]\n",
        "categories_to_keep = ['full_active', 'high_active', 'middle_active','low_active']\n",
        "\n",
        "action = action[action['user_active_degree'].isin(categories_to_keep)]\n",
        "\n",
        "category_mapping = {\n",
        "    'low_active': 0,\n",
        "    'middle_active': 1,\n",
        "    'high_active': 2,\n",
        "    'full_active': 3\n",
        "}\n",
        "\n",
        "action['user_active_degree_encoded'] = action['user_active_degree'].map(category_mapping)\n",
        "action['music_id_encoded'], _ = pd.factorize(action['music_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([25492,  7015,     5,    15,     4,  6031,  6871,  7015])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_X = action[['user_id','video_id','duration_level','tab','user_active_degree_encoded','author_id','music_id_encoded','video_id']]#select_label]]\n",
        "fields = data_X.max().values + 1 # 模型输入的feature_fields\n",
        "fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train= action.groupby('user_id').apply(lambda x: x[:int(len(x)*0.6)]).reset_index(drop=True)\n",
        "valid= action.groupby('user_id').apply(lambda x: x[int(len(x)*0.6):int(len(x)*0.8)]).reset_index(drop=True)\n",
        "test= action.groupby('user_id').apply(lambda x: x[int(len(x)*0.8):]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uTeoDcHyOkVH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:21:03.591352: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.591516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.591583: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.591681: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.591744: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.591793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "seed_tensorflow(seed=para['seed'])\n",
        "\"\"\"\n",
        "    uid_lay = get_layer((1,),'uid',d1 = max(action['user_id'])+1,d2 = para['embedding_dim'],trainable=True)\n",
        "    videoid_lay = get_layer((1,),'video_id',d1 = max(action['video_id'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "    duration_lay = get_layer((1,),'duration_id',d1 = max(action['duration_level'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "    tab_lay = get_layer((1,),'tab',d1 = max(action['tab'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "    user_active_degree_encoded = get_layer((1,),'user_active_degree_encoded',d1 = max(action['user_active_degree_encoded'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "    author_lay = get_layer((1,),'author_id',d1 = max(action['author_id'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "    music_lay = get_layer((1,),'music_id_encoded',d1 = max(action['music_id_encoded'])+1,d2 = int(para['embedding_dim']),trainable=True)\n",
        "\"\"\"\n",
        "\n",
        "features = ['user_id','video_id','duration_level','tab','user_active_degree_encoded','author_id','music_id_encoded','video_id',select_label]\n",
        "\n",
        "# # short_play binary_train_label effective_play long_play complete_play\n",
        "def get_input(df,is_test=False):\n",
        "    X = []\n",
        "    for f in features:\n",
        "        X.append(df[f].values.reshape(-1,1))\n",
        "    y = [df[select_label].values.reshape(-1,1)]\n",
        "    return X,y\n",
        "\n",
        "X_train,y_train = get_input(train,is_test=False)\n",
        "X_valid,y_valid = get_input(valid,is_test=False)\n",
        "X_test,y_test = get_input(test,is_test=True)\n",
        "X_v,y_v = get_input(test[:4],is_test=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0  1399     4 ...     0  1399     0]\n",
            " [    0  6857     2 ...     1  6857     0]\n",
            " [    0  5590     4 ...     2  5590     0]\n",
            " ...\n",
            " [25491  1799     2 ...   398  1799     0]\n",
            " [25491  1799     2 ...   398  1799     0]\n",
            " [25491  2426     0 ...  1069  2426     1]]\n"
          ]
        }
      ],
      "source": [
        "train_X = np.hstack(X_train)\n",
        "test_X = np.hstack(X_test)\n",
        "valid_X = np.hstack(X_valid)\n",
        "\n",
        "# 打印重构的数组以验证\n",
        "print(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_y = y_train[0].flatten()\n",
        "test_y = y_test[0].flatten()\n",
        "valid_y = y_valid[0].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X[:,-1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4],\n",
              "       [2],\n",
              "       [4],\n",
              "       ...,\n",
              "       [2],\n",
              "       [2],\n",
              "       [0]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X[:,2:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(732591, 9)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DeepCrossNet(Model):\n",
        "    \"\"\"\n",
        "        Deep Cross Network\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_fields, embed_dim, num_layers, mlp_dims, dropout):\n",
        "\n",
        "        super(DeepCrossNet, self).__init__()\n",
        "        self.feature_fields = feature_fields\n",
        "        self.offsets = np.array((0, *np.cumsum(feature_fields)[:-1]), dtype = np.int64)\n",
        "        self.input_dims = sum(feature_fields[:-1]) + 1\n",
        "        self.input_lens = len(feature_fields)\n",
        "\n",
        "\n",
        "        #self.feature_fields = feature_fields\n",
        "        #self.input_dims = sum(feature_fields[:-1]) + 1 + feed_emb.shape[0]# 前五个特征的总维度\n",
        "        #self.input_lens = len(feature_fields)   # 前五个特征的数量\n",
        "\n",
        "\n",
        "\n",
        "        #embedding\n",
        "        self.embedding = layers.Embedding(self.input_dims,\n",
        "                                          embed_dim,\n",
        "                                          input_length = self.input_lens-1)\n",
        "\n",
        "        self.pretrained_embedding = layers.Embedding(\n",
        "                                              input_dim=feed_emb.shape[0],\n",
        "                                              output_dim=feed_emb.shape[1],\n",
        "                                              input_length=1,\n",
        "                                              weights=[feed_emb] ,\n",
        "                                              name='pretrain_embedding',\n",
        "                                              trainable=False)  # 根据需要选择是否训练这层\n",
        "\n",
        "        #DNN\n",
        "        self.embed_out_dim = (len(feature_fields)-1) * embed_dim + feed_emb.shape[1]\n",
        "        input_dim = self.embed_out_dim\n",
        "        self.mlp = tf.keras.Sequential()\n",
        "\n",
        "        self.mlp.add(layers.Dense(256, input_shape = [input_dim,]))\n",
        "        self.mlp.add(layers.Activation('relu'))\n",
        "        self.mlp.add(layers.Dropout(dropout))\n",
        "\n",
        "        self.mlp.add(layers.Dense(128, input_shape = [256,]))\n",
        "        self.mlp.add(layers.Activation('relu'))\n",
        "        self.mlp.add(layers.Dropout(dropout))\n",
        "\n",
        "        self.mlp.add(layers.Dense(64, input_shape = [input_dim,]))\n",
        "        self.mlp.add(layers.Activation('relu'))\n",
        "        #self.mlp.add(layers.Dropout(dropout))\n",
        "\n",
        "\n",
        "        #Cross Net\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #LR\n",
        "        self.lr = layers.Dense(1, input_shape = [mlp_dims[-1]+self.embed_out_dim,])\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "            自定义显式交叉层的参数\n",
        "        \"\"\"\n",
        "\n",
        "        self.cross_w = [\n",
        "            self.add_weight(name = 'w_{}'.format(i),\n",
        "                            shape = (self.embed_out_dim, 1),\n",
        "                            trainable = True,\n",
        "                            initializer = 'random_normal'\n",
        "                            )\n",
        "            for i in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        self.cross_b = [\n",
        "            self.add_weight(name = 'b_{}'.format(i),\n",
        "                            shape = (self.embed_out_dim,),\n",
        "                            initializer = 'random_normal',\n",
        "                            trainable = True)\n",
        "            for i in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "    def call(self, x):\n",
        "        #x = x + self.offsets\n",
        "        # embedding\n",
        "        #x = self.embedding(x)\n",
        "        #x = tf.reshape(x, (-1, self.embed_out_dim))\n",
        "        # = x + self.offsets\n",
        "        # embedding\n",
        "        custom_features = x[:, :-2] + self.offsets[:-1]\n",
        "        pretrained_feature = x[:, -2:-1]\n",
        "\n",
        "        custom_embed = self.embedding(custom_features)\n",
        "        pretrained_embed = self.pretrained_embedding(pretrained_feature)\n",
        "        #x = custom_embed\n",
        "        x = layers.Concatenate(axis=1)([custom_embed, pretrained_embed])\n",
        "        x = tf.reshape(x, (-1, self.embed_out_dim))\n",
        "\n",
        "        #DNN out\n",
        "        mlp_part = self.mlp(x)\n",
        "\n",
        "        #Cross Net out\n",
        "        x0 = x\n",
        "        cross = x\n",
        "        for i in range(self.num_layers):\n",
        "            xw = tf.tensordot(cross, self.cross_w[i], axes = [1,0])\n",
        "            cross = x0 * xw + self.cross_b[i] + cross\n",
        "\n",
        "        #Concat output\n",
        "        out = tf.concat([cross, mlp_part], axis = 1)\n",
        "        #tf.print(out.shape)\n",
        "        #LR predict\n",
        "        #out = self.lr(out)\n",
        "        #out = tf.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:21:03.694669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.694856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.694925: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.695018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.695079: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.695128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "base_model_backbone = DeepCrossNet(feature_fields=fields, embed_dim=64, num_layers=3, mlp_dims=[256, 128,64], dropout=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 9)]               0         \n",
            "                                                                 \n",
            " deep_cross_net (DeepCrossN  (None, 576)               3532288   \n",
            " et)                                                             \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 577       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3532865 (13.48 MB)\n",
            "Trainable params: 3083905 (11.76 MB)\n",
            "Non-trainable params: 448960 (1.71 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Dense,Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "inputs = Input(shape=(9,), dtype='int32')\n",
        "duration_id_input = Lambda(lambda x: x[:, 2:3], name='duration_id_extractor')(inputs)\n",
        "labels = Lambda(lambda x: x[:, -1:], name='labels')(inputs)\n",
        "\n",
        "# 将输入层传递给自定义模型\n",
        "outputs = base_model_backbone(inputs)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(outputs)\n",
        "# 创建包含输入层和输出层的完整模型\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# 查看模型结构\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Prototypes(tf.keras.layers.Layer):\n",
        "    name = 'prototypes'\n",
        "    def __init__(self,\n",
        "                 k ,\n",
        "                 beta1=0.0,\n",
        "                 beta2=0.0,\n",
        "                 beta3=0.0,\n",
        "                 init_prototypes = None,\n",
        "                 **kwargs):\n",
        "        super(Prototypes, self).__init__(**kwargs)\n",
        "        self.k = k\n",
        "        self.beta1, self.beta2, self.beta3 = beta1, beta2, beta3\n",
        "        # y_train[0]\n",
        "        self.init_prototypes = init_prototypes\n",
        "        self.binary_classifiers = []\n",
        "        # \n",
        "        for _ in range(5):\n",
        "            mlp = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "            mlp.build((None, 576))\n",
        "            self.binary_classifiers.append(mlp)\n",
        "            \n",
        "    # define prototype\n",
        "    def build(self, input_shape):\n",
        "        self.prototypes = self.add_weight(\n",
        "            name='prototypes',\n",
        "            shape=(1, self.k, 576),\n",
        "            initializer=tf.keras.initializers.Constant(self.init_prototypes[tf.newaxis, :, :]),\n",
        "            trainable=True\n",
        "        )\n",
        "        super(Prototypes, self).build(input_shape)\n",
        "\n",
        "    \n",
        "    def orthogonality_loss(self,prototypes, k):\n",
        "        # 确保传入的 k 是偶数\n",
        "        assert k % 2 == 0, \"k must be even.\"\n",
        "        D = k // 2\n",
        "\n",
        "        prototypes = tf.reshape(prototypes, [k, 576])\n",
        "        prototypes = tf.nn.l2_normalize(prototypes, axis=1)\n",
        "\n",
        "        cosine_similarity_matrix = tf.matmul(prototypes, prototypes, transpose_b=True)\n",
        "\n",
        "        upper_triangular_part = tf.linalg.band_part(cosine_similarity_matrix, 0, -1)\n",
        "        upper_triangular_part -= tf.linalg.band_part(cosine_similarity_matrix, 0, 0)  # 去掉对角线\n",
        "\n",
        "        loss = tf.reduce_sum(tf.square(upper_triangular_part))\n",
        "        normalization_factor = D * (2 * D - 1)\n",
        "        loss /= normalization_factor\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def contrastive_loss(self, vec, labels, duration):\n",
        "        # Normalize the vectors\n",
        "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
        "        #print(\"vec_norm\",vec_norm)\n",
        "        sim_matrix = tf.matmul(vec_norm, vec_norm, transpose_b=True)\n",
        "        #print(\"sim_matrix\",sim_matrix)\n",
        "        # Create masks\n",
        "        label_eq = tf.equal(labels, tf.transpose(labels))\n",
        "        #print(\"label_eq\",label_eq)\n",
        "        duration_eq = tf.equal(duration, tf.transpose(duration))\n",
        "        #print(\"duration_eq\",duration_eq)\n",
        "        mask = tf.logical_and(label_eq, duration_eq)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        #print(\"mask\",mask)\n",
        "\n",
        "        # Compute the softmax denominator\n",
        "        exp_sim = tf.exp(sim_matrix)\n",
        "        total_sum = tf.reduce_sum(exp_sim)\n",
        "        #print(\"total_sum\",total_sum)\n",
        "        normal_sim = exp_sim / total_sum\n",
        "        #print(\"normal_sim\",normal_sim)\n",
        "\n",
        "        log_normal_sim = tf.math.log(normal_sim)\n",
        "        weighted_log_prob = mask * log_normal_sim\n",
        "\n",
        "        num_ones = tf.reduce_sum(mask)\n",
        "\n",
        "        loss = -tf.reduce_sum(weighted_log_prob)\n",
        "        return loss/num_ones\n",
        "\n",
        "    def assign_loss(self,output, vec):\n",
        "\n",
        "\n",
        "        output_norm = tf.nn.l2_normalize(output, axis=1)\n",
        "        vec_norm = tf.nn.l2_normalize(vec, axis=1)\n",
        "\n",
        "        cosine_similarity = tf.reduce_sum(tf.multiply(output_norm, vec_norm), axis=1)\n",
        "        loss = 1 - tf.reduce_mean(cosine_similarity)\n",
        "        return loss\n",
        "    def get_alpha(self,n_batch,duration_one_hot_expanded,alpha_product):\n",
        "        \"\"\"\n",
        "        alpha_product : (n,10,1)\n",
        "        duration_one_hot_expanded: (n,5,1)\n",
        "        \"\"\"\n",
        "        expanded_alpha = tf.reshape(alpha_product, [n_batch, 5, 2, 1])\n",
        "        masked = expanded_alpha * tf.cast(duration_one_hot_expanded, tf.float32)[:, :, :, tf.newaxis]\n",
        "        # n,2,1\n",
        "        alpha_para = tf.reduce_sum(masked, axis=1)\n",
        "        alpha_neg = alpha_para[:, 0, :]  # 第一个元素，保留最后一个轴\n",
        "        alpha_pos = alpha_para[:, 1, :]  # 第二个元素，保留最后一个轴\n",
        "        return alpha_neg,alpha_pos\n",
        "    def mse_loss(self,vec,label):\n",
        "        vec = tf.convert_to_tensor(vec, dtype=tf.float32)\n",
        "\n",
        "        label = tf.convert_to_tensor(label, dtype=tf.float32)\n",
        "\n",
        "        squeezed_label = tf.squeeze(label)\n",
        "\n",
        "        #tf.print(label, summarize=-1)\n",
        "        positive_indices = tf.where(squeezed_label == 1)\n",
        "        negative_indices = tf.where(squeezed_label == 0)\n",
        "\n",
        "        positive_samples = tf.gather(vec, positive_indices)\n",
        "        negative_samples = tf.gather(vec, negative_indices)\n",
        "        positive_samples = tf.squeeze(positive_samples,axis=1)\n",
        "        negative_samples = tf.squeeze(negative_samples,axis=1)\n",
        "        positive_loss = tf.reduce_mean(tf.square(positive_samples - self.pos_mean_prototype))\n",
        "        negative_loss = tf.reduce_mean(tf.square(negative_samples - self.neg_mean_prototype))\n",
        "\n",
        "        total_loss = positive_loss * 0.7 + negative_loss * 0.3\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def binary_loss(self,y_true, y_pred):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "        loss = bce(y_true, y_pred)\n",
        "        return loss     \n",
        "    \n",
        "    def create_mask(self,duration, batch_size, num_classes=40, bucket_size=8):\n",
        "        start_indices = duration * bucket_size\n",
        "        masks = []\n",
        "        for i in range(batch_size):\n",
        "            mask = tf.concat([\n",
        "                tf.zeros((start_indices[i, 0], 1), dtype=tf.float32),\n",
        "                tf.ones((bucket_size, 1), dtype=tf.float32),\n",
        "                tf.zeros((num_classes - start_indices[i, 0] - bucket_size, 1), dtype=tf.float32)\n",
        "            ], axis=0)\n",
        "            masks.append(mask)\n",
        "        return tf.stack(masks)\n",
        "    def binary_crossentropy_manual(self, y_true, y_pred,para):\n",
        "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "        # 防止 log(0) 的情况发生\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
        "\n",
        "        weights = tf.where(y_true == 1, para, 1.0)\n",
        "        weighted_loss = loss * weights\n",
        "\n",
        "        positive_loss = tf.boolean_mask(weighted_loss, y_true == 1)\n",
        "        negative_loss = tf.boolean_mask(weighted_loss, y_true == 0)\n",
        "        #total_positive_loss = tf.reduce_sum(positive_loss)\n",
        "        #total_negative_loss = tf.reduce_sum(negative_loss)\n",
        "\n",
        "        return tf.reduce_mean(weighted_loss)\n",
        "    def prototype_loss(self,prototypes_output):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "        labels = tf.constant([[0], [1], [0], [1], [0], [1], [0], [1], [0], [1]], dtype=tf.float32)\n",
        "        loss = bce(labels, prototypes_output)\n",
        "        return loss\n",
        "    \n",
        "    def call(self, x, training=None):\n",
        "\n",
        "        vec,duration,label = x\n",
        "        #vec = x\n",
        "        #tf.print(vec.shape)\n",
        "        #tf.print(duration.shape)\n",
        "        #tf.print(label.shape)\n",
        "        a = tf.expand_dims(vec, -2)\n",
        "\n",
        "        b = self.prototypes\n",
        "        n_batch = tf.shape(vec)[0]\n",
        "\n",
        "        dot_product = tf.multiply(a, b)\n",
        "\n",
        "        norm_a = tf.norm(a, axis=-1, keepdims=True)  # 保持维度，使得维度是 [batch_size, 1, 1]\n",
        "\n",
        "        norm_b = tf.norm(b, axis=-1, keepdims=True)  # 保持维度，使得维度是 [1, k, 1]\n",
        "\n",
        "        # (batch_size, k,1)\n",
        "        sum_product = tf.reduce_sum(dot_product, axis=-1, keepdims=True)\n",
        "        cos_similarity = sum_product / (norm_a * norm_b)\n",
        "        \n",
        "        #tf.print(cos_similarity.shape)\n",
        "        \"\"\"\n",
        "        tf.print(vec,summarize=-1)\n",
        "        tf.print(self.prototypes,summarize=-1)\n",
        "        tf.print(\"Input duration\")\n",
        "        tf.print(duration.shape)\n",
        "        tf.print(\"Input label\")\n",
        "        tf.print(label.shape)\n",
        "        #tf.print(vec)        \"\"\"\"\"\"\n",
        "\n",
        "        tf.print(norm_a.shape)\n",
        "        tf.print(norm_a)\n",
        "\n",
        "        tf.print(norm_b.shape)\n",
        "        tf.print(norm_b)\n",
        "\n",
        "        tf.print(cos_similarity, summarize=-1)\n",
        "        \"\"\"\n",
        "        # （batch_size, k//2, 2, 1)\n",
        "        reshaped_sum_product = tf.reshape(cos_similarity, (n_batch, self.k // 2, 2))\n",
        "\n",
        "        temperature = 0.05\n",
        "\n",
        "        scaled_logits = reshaped_sum_product / temperature\n",
        "\n",
        "        softmaxed_sum_product = tf.nn.softmax(scaled_logits, axis=2)\n",
        "\n",
        "        # (batch_size, k,1)\n",
        "        alpha_product = tf.reshape(softmaxed_sum_product, (n_batch, self.k, 1))\n",
        "        #tf.print(alpha_product,summarize=-1)\n",
        "        \"\"\"\n",
        "        tf.print(alpha_product.shape)\n",
        "        tf.print(alpha_product)\n",
        "        \"\"\"\n",
        "        #self.new = alpha_product\n",
        "        # n_batch, self.k, 64\n",
        "        # b:(1, k, 64)\n",
        "        alpha_product_expanded = tf.broadcast_to(alpha_product, [n_batch, self.k, 576])\n",
        "\n",
        "        # calculate\n",
        "        weighted_products = alpha_product_expanded * b\n",
        "\n",
        "        # add up\n",
        "        reshaped_weighted_products = tf.reshape(weighted_products, [n_batch, self.k//2, 2, 576])\n",
        "\n",
        "        # (batchsize,5,64)\n",
        "        pre_output = tf.reduce_sum(reshaped_weighted_products, axis=2)  # 对第三个维度求和\n",
        "\n",
        "        # 1 -> [0,1,0,0,0]\n",
        "        duration_one_hot = tf.one_hot(duration, depth=5)\n",
        "        duration_one_hot_expanded = tf.expand_dims(duration_one_hot, -1)\n",
        "        #(batch_size, 5, 1)\n",
        "        duration_one_hot_expanded = tf.squeeze(duration_one_hot_expanded, axis=[1])\n",
        "\n",
        "        masked_output = pre_output * duration_one_hot_expanded\n",
        "        # output (batchsize,64)\n",
        "        output = tf.reduce_sum(masked_output, axis=1)\n",
        "\n",
        "\n",
        "        # get_alpha\n",
        "        alpha_neg, alpha_pos = self.get_alpha(n_batch,duration_one_hot_expanded,alpha_product)\n",
        "        #tf.print(alpha_pos,summarize = -1)\n",
        "        #tf.print(alpha_neg,summarize = -1)\n",
        "        #tf.print(alpha_pos,summarize   =-1)\n",
        "        #tf.print(label, summarize=-1)\n",
        "        classifier_outputs = []\n",
        "\n",
        "\n",
        "        \n",
        "        for i in range(5):\n",
        "            classifier_output = self.binary_classifiers[i](tf.squeeze(self.prototypes,axis=0))\n",
        "             # (batch_size, 1, 1)#\n",
        "            #classifier_output += i\n",
        "            classifier_outputs.append(classifier_output)\n",
        "        \n",
        "        #tf.print(classifier_outputs,summarize = -1)\n",
        "        #tf.print(classifier_outputs.shape,summarize = -1)\n",
        "        indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
        "        prototype_output_elements = []\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            tensor_index = i // 2  \n",
        "            element = tf.gather(classifier_outputs[tensor_index], idx, axis=0) \n",
        "            prototype_output_elements.append(element)\n",
        "\n",
        "        prototype_output = tf.concat(prototype_output_elements, axis=0)\n",
        "        prototype_output = tf.expand_dims(prototype_output,axis=1)\n",
        "        #(1,10)\n",
        "        #tf.print(prototype_output)\n",
        "\n",
        "        positive_indices = 2 * duration + 1\n",
        "        negative_indices = 2 * duration\n",
        "\n",
        "        positive_indices = tf.clip_by_value(positive_indices, 0, prototype_output.shape[0] - 1)\n",
        "        negative_indices = tf.clip_by_value(negative_indices, 0, prototype_output.shape[0] - 1)\n",
        "        #tf.print(alpha_pos,summarize = -1)\n",
        "        # 提取对应的 prototype_output 值\n",
        "        positive_values = tf.gather(prototype_output, positive_indices)\n",
        "        negative_values = tf.gather(prototype_output, negative_indices)\n",
        "        #positive_values = tf.squeeze(positive_values,axis=0)\n",
        "        #negative_values = tf.squeeze(negative_values,axis=0)\n",
        "        output = alpha_pos * tf.squeeze(positive_values,axis=-1) \n",
        "        assign_loss = self.assign_loss(output,vec)\n",
        "        #tf.print(label,summarize = -1)\n",
        "        #tf.print(alpha_pos,summarize = -1)\n",
        "        new = self.binary_loss(label,output)\n",
        "        assign = self.binary_loss(label,alpha_pos)\n",
        "        self.add_loss(0.1 * assign)\n",
        "\n",
        "        ortho_loss = self.orthogonality_loss(self.prototypes,self.k)\n",
        "        self.add_loss(0.3 * ortho_loss)\n",
        "\n",
        "        contrastive_loss = self.contrastive_loss(vec,label,duration)\n",
        "        self.add_loss(0.12 * contrastive_loss)\n",
        "\n",
        "        proto_loss = self.prototype_loss(prototype_output)\n",
        "        self.add_loss(0.1 * proto_loss)\n",
        "        #tf.print(new)\n",
        "        #tf.print(assign)\n",
        "        #tf.print(ortho_loss)\n",
        "        #tf.print(contrastive_loss)\n",
        "        #tf.print(proto_loss)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense,Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "base_model_backbone = DeepCrossNet(feature_fields=fields, embed_dim=64, num_layers=3, mlp_dims=[256, 128,64], dropout=0.5)\n",
        "inputs = Input(shape=(9,), dtype='int32')\n",
        "duration_id_input = Lambda(lambda x: x[:, 2:3], name='duration_id_extractor')(inputs)\n",
        "labels = Lambda(lambda x: x[:, -1:], name='labels')(inputs)\n",
        "\n",
        "# 将输入层传递给自定义模型\n",
        "outputs = base_model_backbone(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 576) dtype=float32 (created by layer 'deep_cross_net_1')>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "proto_tensor = tf.random.normal([10, 576], mean=0.0, stddev=1.0, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:21:03.990092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.990300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.990426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.990668: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.990805: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:03.990903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-28 14:21:04.335262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.335451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.335521: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.335613: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.335678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.335726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "seed_tensorflow(seed=para['seed'])\n",
        "\n",
        "flag = 1\n",
        "if flag:\n",
        "    prototypes_layer = Prototypes(k=10,init_prototypes = proto_tensor\n",
        "                                  )\n",
        "\n",
        "    output = prototypes_layer([outputs,duration_id_input,labels])\n",
        "    #output = prototypes_layer([outputs])\n",
        "    #output.shape\n",
        "else:\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(backbone.outputs[0])\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=[output])\n",
        "\n",
        "seed_tensorflow(seed=para['seed'])\n",
        "\n",
        "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
        "model.compile(\n",
        "    optimizer=adam,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.AUC(name='auc')]  # 确保度量名称与回调中使用的名称匹配\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:21:04.354197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.354364: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.354434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.354525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.354587: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:21:04.354636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-28 14:21:06.644829: I external/local_xla/xla/service/service.cc:168] XLA service 0x7c4fa411a350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-07-28 14:21:06.644845: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
            "2024-07-28 14:21:06.648265: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-07-28 14:21:06.658434: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722147666.700266  590484 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1431/1431 [==============================] - 24s 15ms/step - loss: 2.1882 - auc: 0.6260 - val_loss: 2.1265 - val_auc: 0.6718\n",
            "Epoch 2/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 2.1325 - auc: 0.6669 - val_loss: 2.0969 - val_auc: 0.7001\n",
            "Epoch 3/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 2.1119 - auc: 0.6907 - val_loss: 2.0813 - val_auc: 0.7199\n",
            "Epoch 4/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 2.0976 - auc: 0.7103 - val_loss: 2.0706 - val_auc: 0.7340\n",
            "Epoch 5/200\n",
            "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0867 - auc: 0.7251 - val_loss: 2.0629 - val_auc: 0.7437\n",
            "Epoch 6/200\n",
            "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0785 - auc: 0.7358 - val_loss: 2.0571 - val_auc: 0.7502\n",
            "Epoch 7/200\n",
            "1431/1431 [==============================] - 14s 9ms/step - loss: 2.0717 - auc: 0.7439 - val_loss: 2.0531 - val_auc: 0.7541\n",
            "Epoch 8/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 2.0662 - auc: 0.7498 - val_loss: 2.0498 - val_auc: 0.7569\n",
            "Epoch 9/200\n",
            "1431/1431 [==============================] - 15s 11ms/step - loss: 2.0610 - auc: 0.7548 - val_loss: 2.0468 - val_auc: 0.7589\n",
            "Epoch 10/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 2.0567 - auc: 0.7585 - val_loss: 2.0442 - val_auc: 0.7605\n",
            "Epoch 11/200\n",
            "1431/1431 [==============================] - 11s 7ms/step - loss: 2.0519 - auc: 0.7623 - val_loss: 2.0408 - val_auc: 0.7624\n",
            "Epoch 12/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 2.0474 - auc: 0.7655 - val_loss: 2.0378 - val_auc: 0.7640\n",
            "Epoch 13/200\n",
            "1431/1431 [==============================] - 10s 7ms/step - loss: 2.0425 - auc: 0.7689 - val_loss: 2.0347 - val_auc: 0.7653\n",
            "Epoch 14/200\n",
            "1431/1431 [==============================] - 12s 8ms/step - loss: 2.0379 - auc: 0.7718 - val_loss: 2.0311 - val_auc: 0.7670\n",
            "Epoch 15/200\n",
            "1431/1431 [==============================] - 12s 8ms/step - loss: 2.0329 - auc: 0.7749 - val_loss: 2.0274 - val_auc: 0.7688\n",
            "Epoch 16/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 2.0276 - auc: 0.7784 - val_loss: 2.0233 - val_auc: 0.7709\n",
            "Epoch 17/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 2.0222 - auc: 0.7821 - val_loss: 2.0188 - val_auc: 0.7735\n",
            "Epoch 18/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 2.0164 - auc: 0.7861 - val_loss: 2.0146 - val_auc: 0.7759\n",
            "Epoch 19/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 2.0103 - auc: 0.7906 - val_loss: 2.0100 - val_auc: 0.7791\n",
            "Epoch 20/200\n",
            "1431/1431 [==============================] - 15s 11ms/step - loss: 2.0044 - auc: 0.7949 - val_loss: 2.0056 - val_auc: 0.7822\n",
            "Epoch 21/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9984 - auc: 0.7993 - val_loss: 2.0016 - val_auc: 0.7847\n",
            "Epoch 22/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9925 - auc: 0.8038 - val_loss: 1.9981 - val_auc: 0.7871\n",
            "Epoch 23/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9870 - auc: 0.8077 - val_loss: 1.9953 - val_auc: 0.7888\n",
            "Epoch 24/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9817 - auc: 0.8115 - val_loss: 1.9926 - val_auc: 0.7902\n",
            "Epoch 25/200\n",
            "1431/1431 [==============================] - 14s 9ms/step - loss: 1.9773 - auc: 0.8144 - val_loss: 1.9908 - val_auc: 0.7914\n",
            "Epoch 26/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9729 - auc: 0.8175 - val_loss: 1.9891 - val_auc: 0.7920\n",
            "Epoch 27/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9694 - auc: 0.8196 - val_loss: 1.9875 - val_auc: 0.7928\n",
            "Epoch 28/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9657 - auc: 0.8219 - val_loss: 1.9864 - val_auc: 0.7931\n",
            "Epoch 29/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9623 - auc: 0.8240 - val_loss: 1.9854 - val_auc: 0.7934\n",
            "Epoch 30/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9593 - auc: 0.8258 - val_loss: 1.9844 - val_auc: 0.7937\n",
            "Epoch 31/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9562 - auc: 0.8278 - val_loss: 1.9836 - val_auc: 0.7939\n",
            "Epoch 32/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9537 - auc: 0.8292 - val_loss: 1.9829 - val_auc: 0.7942\n",
            "Epoch 33/200\n",
            "1431/1431 [==============================] - 15s 11ms/step - loss: 1.9508 - auc: 0.8310 - val_loss: 1.9825 - val_auc: 0.7943\n",
            "Epoch 34/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9490 - auc: 0.8319 - val_loss: 1.9817 - val_auc: 0.7943\n",
            "Epoch 35/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9467 - auc: 0.8332 - val_loss: 1.9812 - val_auc: 0.7944\n",
            "Epoch 36/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9444 - auc: 0.8346 - val_loss: 1.9808 - val_auc: 0.7944\n",
            "Epoch 37/200\n",
            "1431/1431 [==============================] - 13s 9ms/step - loss: 1.9426 - auc: 0.8355 - val_loss: 1.9806 - val_auc: 0.7942\n",
            "Epoch 38/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9409 - auc: 0.8365 - val_loss: 1.9801 - val_auc: 0.7942\n",
            "Epoch 39/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9389 - auc: 0.8377 - val_loss: 1.9801 - val_auc: 0.7944\n",
            "Epoch 40/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9373 - auc: 0.8385 - val_loss: 1.9799 - val_auc: 0.7940\n",
            "Epoch 41/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9357 - auc: 0.8394 - val_loss: 1.9798 - val_auc: 0.7941\n",
            "Epoch 42/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9341 - auc: 0.8403 - val_loss: 1.9796 - val_auc: 0.7939\n",
            "Epoch 43/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9325 - auc: 0.8412 - val_loss: 1.9796 - val_auc: 0.7936\n",
            "Epoch 44/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9311 - auc: 0.8419 - val_loss: 1.9796 - val_auc: 0.7935\n",
            "Epoch 45/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9300 - auc: 0.8424 - val_loss: 1.9794 - val_auc: 0.7934\n",
            "Epoch 46/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9287 - auc: 0.8431 - val_loss: 1.9795 - val_auc: 0.7934\n",
            "Epoch 47/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9275 - auc: 0.8437 - val_loss: 1.9795 - val_auc: 0.7930\n",
            "Epoch 48/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9261 - auc: 0.8444 - val_loss: 1.9797 - val_auc: 0.7932\n",
            "Epoch 49/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9247 - auc: 0.8452 - val_loss: 1.9795 - val_auc: 0.7926\n",
            "Epoch 50/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9237 - auc: 0.8457 - val_loss: 1.9796 - val_auc: 0.7927\n",
            "Epoch 51/200\n",
            "1431/1431 [==============================] - 15s 10ms/step - loss: 1.9228 - auc: 0.8461 - val_loss: 1.9796 - val_auc: 0.7924\n",
            "Epoch 52/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9216 - auc: 0.8467 - val_loss: 1.9796 - val_auc: 0.7924\n",
            "Epoch 53/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9203 - auc: 0.8474 - val_loss: 1.9796 - val_auc: 0.7921\n",
            "Epoch 54/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9192 - auc: 0.8479 - val_loss: 1.9796 - val_auc: 0.7917\n",
            "Epoch 55/200\n",
            "1431/1431 [==============================] - 14s 10ms/step - loss: 1.9182 - auc: 0.8484 - val_loss: 1.9795 - val_auc: 0.7918\n"
          ]
        }
      ],
      "source": [
        "seed_tensorflow(seed=para['seed'])\n",
        "\n",
        "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
        "\n",
        "model.compile(optimizer=adam,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['AUC'])\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证集损失\n",
        "    mode='min',          # 最小化验证集损失\n",
        "    patience=para['callback']['patience']  # 等待的轮数\n",
        ")\n",
        "\n",
        "checkpoint_auc = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=para['callback']['CPF_model'],  # 保存模型的路径\n",
        "    monitor='val_auc',  # 监控验证集 AUC\n",
        "    mode='max',          # 保存验证 AUC 最大的模型\n",
        "    save_weights_only=True,  # 只保存模型权重\n",
        "    save_best_only=True  # 只保存最佳模型\n",
        ")\n",
        "\n",
        "\n",
        "hist = model.fit(train_X,\n",
        "                 train_y,\n",
        "                 epochs=200,\n",
        "                 batch_size=512,\n",
        "                 shuffle=True,\n",
        "                 verbose=para['verbose'],\n",
        "                 callbacks=[checkpoint_auc,es_callback],\n",
        "                 validation_data=(valid_X,valid_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-28 14:33:50.869484: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:33:50.869783: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:33:50.869882: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:33:50.869978: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:33:50.870042: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-28 14:33:50.870092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 0s 6ms/step\n",
            "auc: 0.7879866\n",
            "logloss: 0.47016987\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# infer\n",
        "\"\"\"\n",
        "0.78959733\n",
        "0.46864867\n",
        "\"\"\"\n",
        "\n",
        "seed_tensorflow(seed=para['seed'])\n",
        "model.load_weights(para['callback']['CPF_model'])\n",
        "y_test_pred = model.predict(test_X,batch_size=10000)\n",
        "auc_metric = tf.keras.metrics.AUC()\n",
        "auc_metric.update_state(y_test[0], y_test_pred)\n",
        "auc = auc_metric.result().numpy()\n",
        "\n",
        "logloss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
        "\n",
        "logloss_metric.update_state(y_test[0], y_test_pred)\n",
        "log_loss = logloss_metric.result().numpy()\n",
        "print('auc:',auc)\n",
        "print('logloss:',log_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
